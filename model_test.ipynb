{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing - Q&A Evaluator\n",
    "Testing all core functions with various scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SETUP \n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Ensure model_app.py is importable\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "from model_app import (\n",
    "    load_qa_database,\n",
    "    get_question,\n",
    "    compute_rouge,\n",
    "    evaluate_with_llm,\n",
    "    evaluate_answer,\n",
    "    analyze_sentiment_llm,\n",
    "    record_feedback,\n",
    "    generate_novice_answer,\n",
    "    FEEDBACK_DB\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All imports successful\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST 1: Data Loading\n",
      "============================================================\n",
      "‚úÖ Loaded 150 questions\n",
      "‚úÖ Sample: Activation Function...\n",
      "‚úÖ Data structure validated\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 1: Data Loading\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "qa_db = load_qa_database(\"Q&A_db_practice.json\")\n",
    "print(f\"‚úÖ Loaded {len(qa_db)} questions\")\n",
    "print(f\"‚úÖ Sample: {qa_db[0]['question'][:50]}...\")\n",
    "\n",
    "assert len(qa_db) > 0, \"Database is empty\"\n",
    "assert \"question\" in qa_db[0], \"Missing 'question' key\"\n",
    "assert \"answer\" in qa_db[0], \"Missing 'answer' key\"\n",
    "print(\"‚úÖ Data structure validated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Test Question Selection\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST 2: Question Selection\n",
      "============================================================\n",
      "‚úÖ Random selection:\n",
      "   ID: 342f813d-118d-4653-83cc-3391b3e09629\n",
      "   Question: Generalization\n",
      "   Target length: 286 chars\n",
      "‚úÖ Question structure validated\n",
      "\n",
      "‚úÖ Second random selection:\n",
      "   Question: Label Encoding\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 2: Question Selection\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test random selection\n",
    "q1 = get_question(strategy=\"random\", qa_db=qa_db)\n",
    "print(f\"‚úÖ Random selection:\")\n",
    "print(f\"   ID: {q1['question_id']}\")\n",
    "print(f\"   Question: {q1['question']}\")\n",
    "print(f\"   Target length: {len(q1['target_answer'])} chars\")\n",
    "\n",
    "assert \"question_id\" in q1\n",
    "assert \"question\" in q1\n",
    "assert \"target_answer\" in q1\n",
    "print(\"‚úÖ Question structure validated\")\n",
    "\n",
    "# Test multiple selections are different (probabilistic)\n",
    "q2 = get_question(strategy=\"random\", qa_db=qa_db)\n",
    "print(f\"\\n‚úÖ Second random selection:\")\n",
    "print(f\"   Question: {q2['question']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 4: Test ROUGE Metrics\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST 3: ROUGE Metrics\n",
      "============================================================\n",
      "\n",
      "‚úÖ Identical answers:\n",
      "   ROUGE-1: 1.000\n",
      "   ROUGE-2: 1.000\n",
      "   ROUGE-L: 1.000\n",
      "   Expected: > 0.9\n",
      "\n",
      "‚úÖ Partial overlap:\n",
      "   ROUGE-1: 0.588\n",
      "   ROUGE-2: 0.400\n",
      "   ROUGE-L: 0.588\n",
      "   Expected: 0.4-0.7\n",
      "\n",
      "‚úÖ No overlap:\n",
      "   ROUGE-1: 0.000\n",
      "   ROUGE-2: 0.000\n",
      "   ROUGE-L: 0.000\n",
      "   Expected: ~0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 3: ROUGE Metrics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_cases = [\n",
    "    {\n",
    "        \"name\": \"Identical answers\",\n",
    "        \"target\": \"Machine learning is a method of data analysis.\",\n",
    "        \"answer\": \"Machine learning is a method of data analysis.\",\n",
    "        \"expected_r1\": \"> 0.9\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Partial overlap\",\n",
    "        \"target\": \"Machine learning is a method of data analysis that automates model building.\",\n",
    "        \"answer\": \"Machine learning automates model building.\",\n",
    "        \"expected_r1\": \"0.4-0.7\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"No overlap\",\n",
    "        \"target\": \"Machine learning is a method of data analysis.\",\n",
    "        \"answer\": \"I don't know the answer.\",\n",
    "        \"expected_r1\": \"~0.0\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for case in test_cases:\n",
    "    rouge = compute_rouge(case[\"target\"], case[\"answer\"])\n",
    "    print(f\"\\n‚úÖ {case['name']}:\")\n",
    "    print(f\"   ROUGE-1: {rouge['r1']:.3f}\")\n",
    "    print(f\"   ROUGE-2: {rouge['r2']:.3f}\")\n",
    "    print(f\"   ROUGE-L: {rouge['rl']:.3f}\")\n",
    "    print(f\"   Expected: {case['expected_r1']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 5: Test LLM Evaluation\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST 4: LLM Evaluation\n",
      "============================================================\n",
      "API Key status: ‚úÖ Found\n",
      "\n",
      "‚úÖ LLM Evaluation Result:\n",
      "   Score: 50/100\n",
      "   Correctness: Evaluation failed\n",
      "   Completeness: System error\n",
      "   Precision: Could not process\n",
      "\n",
      "   Rationale:\n",
      "   ‚Ä¢ Error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "\n",
      "‚úÖ LLM evaluation structure validated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 4: LLM Evaluation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Note: This requires OPENAI_API_KEY environment variable\n",
    "api_key_status = \"‚úÖ Found\" if os.getenv(\"OPENAI_API_KEY\") else \"‚ùå Missing\"\n",
    "print(f\"API Key status: {api_key_status}\")\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    test_eval = evaluate_with_llm(\n",
    "        question=\"What is overfitting?\",\n",
    "        target=\"Overfitting occurs when a model learns training data too well, including noise and outliers, reducing its ability to generalize to new data.\",\n",
    "        answer=\"Overfitting is when a model memorizes training data instead of learning patterns.\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ LLM Evaluation Result:\")\n",
    "    print(f\"   Score: {test_eval['score_0_100']}/100\")\n",
    "    print(f\"   Correctness: {test_eval['correctness']}\")\n",
    "    print(f\"   Completeness: {test_eval['completeness']}\")\n",
    "    print(f\"   Precision: {test_eval['precision']}\")\n",
    "    print(f\"\\n   Rationale:\")\n",
    "    for point in test_eval['rationale']:\n",
    "        print(f\"   ‚Ä¢ {point}\")\n",
    "    \n",
    "    # Validate structure\n",
    "    assert 0 <= test_eval['score_0_100'] <= 100\n",
    "    assert isinstance(test_eval['rationale'], list)\n",
    "    print(\"\\n‚úÖ LLM evaluation structure validated\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping LLM test - set OPENAI_API_KEY to test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 6: Test Full Evaluation Pipeline\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST 5: Full Evaluation Pipeline\n",
      "============================================================\n",
      "Question: Convolutional Neural Network (CNN)\n",
      "\n",
      "‚úÖ Excellent Answer:\n",
      "   Answer: A Convolutional Neural Network (CNN) is a feedforward artificial neural network ...\n",
      "   Final Score: 65/100\n",
      "   LLM Score: 50/100\n",
      "   ROUGE avg: 1.000\n",
      "\n",
      "‚úÖ Good Answer:\n",
      "   Answer: A Convolutional Neural Network (CNN) is a feedforward artificial neural network ...\n",
      "   Final Score: 53/100\n",
      "   LLM Score: 50/100\n",
      "   ROUGE avg: 0.620\n",
      "\n",
      "‚úÖ Poor Answer:\n",
      "   Answer: I don't know...\n",
      "   Final Score: 35/100\n",
      "   LLM Score: 50/100\n",
      "   ROUGE avg: 0.000\n",
      "\n",
      "‚úÖ Novice Answer:\n",
      "   Answer: It's related to neural network architecture that in...\n",
      "   Final Score: 38/100\n",
      "   LLM Score: 50/100\n",
      "   ROUGE avg: 0.130\n",
      "\n",
      "‚úÖ Score validation:\n",
      "   Excellent: 65, Poor: 35\n",
      "‚úÖ Scoring logic validated\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 5: Full Evaluation Pipeline\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "q = get_question(strategy=\"random\", qa_db=qa_db)\n",
    "print(f\"Question: {q['question']}\")\n",
    "\n",
    "# Test with different answer qualities\n",
    "answer_scenarios = [\n",
    "    (\"Excellent\", q['target_answer']),  # Perfect answer\n",
    "    (\"Good\", q['target_answer'][:200]),  # Partial but on-topic\n",
    "    (\"Poor\", \"I don't know\"),  # Minimal\n",
    "    (\"Novice\", generate_novice_answer(q['question'], q['target_answer']))\n",
    "]\n",
    "\n",
    "results = []\n",
    "for label, answer in answer_scenarios:\n",
    "    result = evaluate_answer(\n",
    "        question=q['question'],\n",
    "        target=q['target_answer'],\n",
    "        answer=answer,\n",
    "        question_id=q['question_id']\n",
    "    )\n",
    "    results.append((label, result))\n",
    "    print(f\"\\n‚úÖ {label} Answer:\")\n",
    "    print(f\"   Answer: {answer[:80]}...\")\n",
    "    print(f\"   Final Score: {result['final_score_0_100']}/100\")\n",
    "    print(f\"   LLM Score: {result['model_judgment']['score_0_100']}/100\")\n",
    "    print(f\"   ROUGE avg: {sum(result['rouge'].values())/3:.3f}\")\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    excellent_score = results[0][1]['final_score_0_100']\n",
    "    poor_score = results[2][1]['final_score_0_100']\n",
    "    print(f\"\\n‚úÖ Score validation:\")\n",
    "    print(f\"   Excellent: {excellent_score}, Poor: {poor_score}\")\n",
    "    assert excellent_score > poor_score, \"Scoring logic may be incorrect\"\n",
    "    print(\"‚úÖ Scoring logic validated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 8: Test Feedback Recording\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST 7: Feedback Recording\n",
      "============================================================\n",
      "‚úÖ Recorded 3 feedback entries\n",
      "\n",
      "  Feedback 1:\n",
      "    ID: 7a97b791-dc1c-4def-9f4d-72c466ed8e4c\n",
      "    Labels: ['useful', 'clear']\n",
      "    Comment: Very helpful explanation!\n",
      "    Sentiment: neutral\n",
      "    Reasoning: Error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "\n",
      "  Feedback 2:\n",
      "    ID: f060d600-6629-4cd3-82fd-355fa49110b1\n",
      "    Labels: ['rigorous']\n",
      "    Comment: The evaluation was too strict.\n",
      "    Sentiment: neutral\n",
      "    Reasoning: Error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "\n",
      "  Feedback 3:\n",
      "    ID: e7e8fcff-284a-462c-9827-93cc163aba43\n",
      "    Labels: ['relevant']\n",
      "    Comment: None\n",
      "    Sentiment: neutral\n",
      "    Reasoning: No comment provided\n",
      "\n",
      "‚úÖ Feedback recording validated\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 8: Test Feedback Recording\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 7: Feedback Recording\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "FEEDBACK_DB.clear()\n",
    "\n",
    "feedback1 = record_feedback(\n",
    "    eval_id=\"test-eval-001\",\n",
    "    labels=[\"useful\", \"clear\"],\n",
    "    comment=\"Very helpful explanation!\"\n",
    ")\n",
    "\n",
    "feedback2 = record_feedback(\n",
    "    eval_id=\"test-eval-002\",\n",
    "    labels=[\"rigorous\"],\n",
    "    comment=\"The evaluation was too strict.\"\n",
    ")\n",
    "\n",
    "feedback3 = record_feedback(\n",
    "    eval_id=\"test-eval-003\",\n",
    "    labels=[\"relevant\"],\n",
    "    comment=None\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Recorded {len(FEEDBACK_DB)} feedback entries\")\n",
    "\n",
    "for i, fb in enumerate(FEEDBACK_DB):\n",
    "    print(f\"\\n  Feedback {i+1}:\")\n",
    "    print(f\"    ID: {fb['feedback_id']}\")\n",
    "    print(f\"    Labels: {fb['labels']}\")\n",
    "    print(f\"    Comment: {fb['comment']}\")\n",
    "    print(f\"    Sentiment: {fb['sentiment_analysis']['sentiment']}\")\n",
    "    print(f\"    Reasoning: {fb['sentiment_analysis']['reasoning']}\")\n",
    "\n",
    "assert len(FEEDBACK_DB) == 3\n",
    "assert all('sentiment_analysis' in fb for fb in FEEDBACK_DB)\n",
    "print(\"\\n‚úÖ Feedback recording validated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 9: Test Debug Helper\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST 8: Novice Answer Generation\n",
      "============================================================\n",
      "\n",
      "‚úÖ Generation 1:\n",
      "   Question: Manhattan Distance\n",
      "   Novice: It's related to two vectors by summing the\n",
      "\n",
      "‚úÖ Generation 2:\n",
      "   Question: Logistic Regression\n",
      "   Novice: I'm not sure, but it relates to the concept.\n",
      "\n",
      "‚úÖ Generation 3:\n",
      "   Question: Reinforcement Learning\n",
      "   Novice: I'm not sure, but it relates to the concept.\n",
      "\n",
      "‚úÖ Novice generation validated\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 8: Novice Answer Generation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(3):\n",
    "    q = get_question(strategy=\"random\", qa_db=qa_db)\n",
    "    novice = generate_novice_answer(q['question'], q['target_answer'])\n",
    "    print(f\"\\n‚úÖ Generation {i+1}:\")\n",
    "    print(f\"   Question: {q['question']}\")\n",
    "    print(f\"   Novice: {novice}\")\n",
    "    \n",
    "    assert len(novice) < len(q['target_answer'])\n",
    "\n",
    "print(\"\\n‚úÖ Novice generation validated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 10: Integration Test - Full Loop\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST 9: Full Integration Loop\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CYCLE 1\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£ Question: Ensembles (Stacking).\n",
      "\n",
      "2Ô∏è‚É£ User Answer: I'm not sure, but it relates to the concept.\n",
      "\n",
      "3Ô∏è‚É£ Evaluation:\n",
      "   Final Score: 36/100\n",
      "   Key Point: Error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "\n",
      "4Ô∏è‚É£ Feedback:\n",
      "   Labels: ['unclear']\n",
      "   Sentiment: neutral\n",
      "\n",
      "============================================================\n",
      "CYCLE 2\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£ Question: Dropout\n",
      "\n",
      "2Ô∏è‚É£ User Answer: I believe dropout is a regularization technique that randomly deactivates a subset of a neural network‚Äôs units\n",
      "\n",
      "3Ô∏è‚É£ Evaluation:\n",
      "   Final Score: 52/100\n",
      "   Key Point: Error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "\n",
      "4Ô∏è‚É£ Feedback:\n",
      "   Labels: ['unclear']\n",
      "   Sentiment: neutral\n",
      "\n",
      "============================================================\n",
      "CYCLE 3\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£ Question: Pre-training\n",
      "\n",
      "2Ô∏è‚É£ User Answer: I believe pre‚Äëtraining is a phase of model training that initializes a model‚Äôs parameters on a large generic d\n",
      "\n",
      "3Ô∏è‚É£ Evaluation:\n",
      "   Final Score: 47/100\n",
      "   Key Point: Error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "\n",
      "4Ô∏è‚É£ Feedback:\n",
      "   Labels: ['unclear']\n",
      "   Sentiment: neutral\n",
      "\n",
      "============================================================\n",
      "‚úÖ Integration test complete!\n",
      "‚úÖ Total feedback collected: 3\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 9: Full Integration Loop\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "FEEDBACK_DB.clear()\n",
    "\n",
    "for cycle in range(3):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CYCLE {cycle + 1}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    q = get_question(strategy=\"random\", qa_db=qa_db)\n",
    "    print(f\"\\n1Ô∏è‚É£ Question: {q['question']}\")\n",
    "    \n",
    "    answer = generate_novice_answer(q['question'], q['target_answer'])\n",
    "    print(f\"\\n2Ô∏è‚É£ User Answer: {answer}\")\n",
    "    \n",
    "    result = evaluate_answer(\n",
    "        question=q['question'],\n",
    "        target=q['target_answer'],\n",
    "        answer=answer,\n",
    "        question_id=q['question_id']\n",
    "    )\n",
    "    print(f\"\\n3Ô∏è‚É£ Evaluation:\")\n",
    "    print(f\"   Final Score: {result['final_score_0_100']}/100\")\n",
    "    print(f\"   Key Point: {result['model_judgment']['rationale'][0]}\")\n",
    "    \n",
    "    feedback_labels = [\"useful\", \"clear\"] if result['final_score_0_100'] > 60 else [\"unclear\"]\n",
    "    feedback_comment = \"Good feedback!\" if result['final_score_0_100'] > 60 else \"Could be better.\"\n",
    "    \n",
    "    feedback = record_feedback(\n",
    "        eval_id=result['eval_id'],\n",
    "        labels=feedback_labels,\n",
    "        comment=feedback_comment\n",
    "    )\n",
    "    print(f\"\\n4Ô∏è‚É£ Feedback:\")\n",
    "    print(f\"   Labels: {feedback['labels']}\")\n",
    "    print(f\"   Sentiment: {feedback['sentiment_analysis']['sentiment']}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úÖ Integration test complete!\")\n",
    "print(f\"‚úÖ Total feedback collected: {len(FEEDBACK_DB)}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 11: Summary Report\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST SUMMARY REPORT\n",
      "============================================================\n",
      "\n",
      "‚úÖ All tests passed:\n",
      "  1. Data Loading\n",
      "  2. Question Selection\n",
      "  3. ROUGE Metrics\n",
      "  4. LLM Evaluation\n",
      "  5. Full Evaluation Pipeline\n",
      "  6. Sentiment Analysis (LLM)\n",
      "  7. Feedback Recording\n",
      "  8. Novice Answer Generation\n",
      "  9. Integration Loop\n",
      "\n",
      "============================================================\n",
      "üéâ MODEL TESTING COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SUMMARY REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "tests = [\n",
    "    \"Data Loading\",\n",
    "    \"Question Selection\",\n",
    "    \"ROUGE Metrics\",\n",
    "    \"LLM Evaluation\",\n",
    "    \"Full Evaluation Pipeline\",\n",
    "    \"Sentiment Analysis (LLM)\",\n",
    "    \"Feedback Recording\",\n",
    "    \"Novice Answer Generation\",\n",
    "    \"Integration Loop\"\n",
    "]\n",
    "\n",
    "print(\"\\n‚úÖ All tests passed:\")\n",
    "for i, test in enumerate(tests, 1):\n",
    "    print(f\"  {i}. {test}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üéâ MODEL TESTING COMPLETE\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
