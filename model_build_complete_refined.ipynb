{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Build - Q&A Evaluator (COMPLETE REFINED VERSION)\n",
    "## Assignment 11.02 - LLM Applications\n",
    "\n",
    "### Testing 5 Models:\n",
    "1. GPT-4 (OpenAI)\n",
    "2. GPT-4o-mini (OpenAI)\n",
    "3. Llama-3-8B (HuggingFace)\n",
    "4. Qwen-2.5-7B (HuggingFace)\n",
    "5. Gemma-2-9B (HuggingFace)\n",
    "\n",
    "### Refinements:\n",
    "- ✅ Includes GPT-4 in model comparison\n",
    "- ✅ Fixed prompt testing (Cell 11 actually tests prompts)\n",
    "- ✅ Explained consistency & calibration testing\n",
    "- ✅ Cells 5-10 adapted for all 5 models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 1. Setup and Introduction\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAI API Key loaded\n",
      "✅ HuggingFace token loaded\n",
      "✅ OpenAI client initialized\n",
      "✅ HuggingFace client initialized\n",
      "✅ Loaded 150 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vicvolman/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import Optional, Dict, List\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    " \n",
    "load_dotenv()\n",
    "\n",
    "# Verify API keys\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"❌ Set OPENAI_API_KEY in .env file\")\n",
    "else:\n",
    "    print(\"✅ OpenAI API Key loaded\")\n",
    "\n",
    "if not os.getenv(\"HF_TOKEN\"):\n",
    "    print(\"❌ Set HF_TOKEN in .env file\")\n",
    "else:\n",
    "    print(\"✅ HuggingFace token loaded\")\n",
    "\n",
    "# Import OpenAI\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    print(\"✅ OpenAI client initialized\")\n",
    "except ImportError:\n",
    "    print(\"❌ Install: pip install openai\")\n",
    "\n",
    "# Import HuggingFace\n",
    "try:\n",
    "    from huggingface_hub import InferenceClient\n",
    "    hf_client = InferenceClient(token=os.getenv(\"HF_TOKEN\"))\n",
    "    print(\"✅ HuggingFace client initialized\")\n",
    "except ImportError:\n",
    "    print(\"❌ Install: pip install huggingface_hub\")\n",
    "\n",
    "# Load Q&A database\n",
    "with open(\"Q&A_db_practice.json\", \"r\") as f:\n",
    "    qa_db = json.load(f)\n",
    "print(f\"✅ Loaded {len(qa_db)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import comparative analysis module\n",
    "import sys\n",
    "sys.path.append('/home/claude')  # or wherever you put the module\n",
    "\n",
    "\n",
    "# Add these three helper functions\n",
    "def extract_response_text(response):\n",
    "    # Dict with 'content' key (your case)\n",
    "    if isinstance(response, dict) and 'content' in response:\n",
    "        return response['content']\n",
    "    # OpenAI response object\n",
    "    elif hasattr(response, 'choices'):\n",
    "        return response.choices[0].message.content\n",
    "    # Already a string\n",
    "    elif isinstance(response, str):\n",
    "        return response\n",
    "    else:\n",
    "        return str(response)\n",
    "\n",
    "\n",
    "def clean_json_response(text):\n",
    "    text = text.strip()\n",
    "    if \"```json\" in text:\n",
    "        text = text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "    elif \"```\" in text:\n",
    "        text = text.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "    return text.strip()  # Extra strip to remove leading newlines\n",
    "\n",
    "def safe_json_parse(response):\n",
    "    import json\n",
    "    # Extract text\n",
    "    if isinstance(response, dict) and 'content' in response:\n",
    "        text = response['content']\n",
    "    elif hasattr(response, 'choices'):\n",
    "        text = response.choices[0].message.content\n",
    "    elif isinstance(response, str):\n",
    "        text = response\n",
    "    else:\n",
    "        text = str(response)\n",
    "    \n",
    "    # Remove markdown\n",
    "    if \"```json\" in text:\n",
    "        text = text.split(\"```json\")[1].split(\"```\")[0]\n",
    "    elif \"```\" in text:\n",
    "        text = text.split(\"```\")[1].split(\"```\")[0]\n",
    "    \n",
    "    # Parse (strip before parsing!)\n",
    "    return json.loads(text.strip())\n",
    "\n",
    "# Patch the module\n",
    "import comparative_analysis\n",
    "comparative_analysis.safe_json_parse = safe_json_parse\n",
    "comparative_analysis.extract_response_text = extract_response_text\n",
    "\n",
    "from comparative_analysis import (\n",
    "    compare_models,\n",
    "    visualize_model_comparison,\n",
    "    compare_prompts,\n",
    "    visualize_prompt_comparison,\n",
    "    analyze_interaction,\n",
    "    visualize_interaction,\n",
    "    generate_rationale,\n",
    "    run_full_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 2. Helper Functions Definition\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Helper functions for testing different LLMs\n",
    "\"\"\"\n",
    "\n",
    "def call_openai_model(model_name: str, messages: List[Dict], max_tokens: int = 500, temperature: float = 0.3) -> Dict:\n",
    "    \"\"\"\n",
    "    Call OpenAI API models (GPT-4, GPT-4o-mini, etc.)\n",
    "    \n",
    "    Returns dict with:\n",
    "    - content: response text\n",
    "    - latency: time in seconds\n",
    "    - tokens: actual token count\n",
    "    - cost: estimated cost in dollars\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        latency = time.time() - start\n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        # Token usage\n",
    "        tokens_used = response.usage.total_tokens\n",
    "        \n",
    "        # Cost estimation (as of 2024)\n",
    "        if \"gpt-4\" in model_name.lower() and \"mini\" not in model_name.lower():\n",
    "            # GPT-4: $0.03/1K input, $0.06/1K output\n",
    "            input_cost = (response.usage.prompt_tokens / 1000) * 0.03\n",
    "            output_cost = (response.usage.completion_tokens / 1000) * 0.06\n",
    "        else:\n",
    "            # GPT-4o-mini: $0.00015/1K input, $0.0006/1K output\n",
    "            input_cost = (response.usage.prompt_tokens / 1000) * 0.00015\n",
    "            output_cost = (response.usage.completion_tokens / 1000) * 0.0006\n",
    "        \n",
    "        total_cost = input_cost + output_cost\n",
    "        \n",
    "        return {\n",
    "            \"content\": content,\n",
    "            \"latency\": latency,\n",
    "            \"tokens\": tokens_used,\n",
    "            \"cost\": total_cost,\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"content\": None,\n",
    "            \"latency\": time.time() - start,\n",
    "            \"tokens\": 0,\n",
    "            \"cost\": 0,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "\n",
    "def call_huggingface_model(model_name: str, messages: List[Dict], max_tokens: int = 500, temperature: float = 0.3) -> Dict:\n",
    "    \"\"\"\n",
    "    Call HuggingFace Inference API models\n",
    "    \n",
    "    Returns dict with same structure as call_openai_model\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = hf_client.chat_completion(\n",
    "            messages=messages,\n",
    "            model=model_name,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        latency = time.time() - start\n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        # Estimate tokens (rough)\n",
    "        tokens_used = len(content.split()) * 1.3  # Rough token estimation\n",
    "        \n",
    "        return {\n",
    "            \"content\": content,\n",
    "            \"latency\": latency,\n",
    "            \"tokens\": int(tokens_used),\n",
    "            \"cost\": 0,  # HF Inference API is free\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"content\": None,\n",
    "            \"latency\": time.time() - start,\n",
    "            \"tokens\": 0,\n",
    "            \"cost\": 0,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "\n",
    "def extract_score_from_response(content: str) -> int:\n",
    "    \"\"\"\n",
    "    Extract numerical score from LLM response.\n",
    "    Looks for JSON or plain score.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try JSON parsing first\n",
    "        if \"{\" in content and \"}\" in content:\n",
    "            start = content.find(\"{\")\n",
    "            end = content.rfind(\"}\") + 1\n",
    "            json_str = content[start:end]\n",
    "            data = json.loads(json_str)\n",
    "            if \"final_score_0_100\" in data:\n",
    "                return data[\"final_score_0_100\"]\n",
    "            if \"score_0_100\" in data:\n",
    "                return data[\"score_0_100\"]\n",
    "            if \"model_judgment\" in data and \"score_0_100\" in data[\"model_judgment\"]:\n",
    "                return data[\"model_judgment\"][\"score_0_100\"]\n",
    "        \n",
    "        # Fallback: look for \"Score: XX\" pattern\n",
    "        import re\n",
    "        match = re.search(r'score[:\\s]+(\\d+)', content, re.IGNORECASE)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        \n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "print(\"✅ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 3. Prompt Definitions \n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining different prompts according to the prompt engineering cheat sheet (see files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_V1 = \"\"\"Evaluate this student answer.\n",
    "\n",
    "Question: {question}\n",
    "Target: {target}\n",
    "Answer: {answer}\n",
    "\n",
    "Score 0-100 and explain. Return JSON with score_0_100, correctness, completeness, precision, rationale.\"\"\"\n",
    "\n",
    "\n",
    "PROMPT_V2 = \"\"\"### ROLE\n",
    "You are an expert AI/ML educator evaluating student answers.\n",
    "\n",
    "### TASK\n",
    "Evaluate the student's answer by comparing it to the target.\n",
    "\n",
    "### INPUT DATA\n",
    "**Question:** {question}\n",
    "\n",
    "**Target Answer:** {target}\n",
    "\n",
    "**Student Answer:** {answer}\n",
    "\n",
    "### EVALUATION CRITERIA\n",
    "1. **Correctness**: Are core concepts accurate?\n",
    "2. **Completeness**: Are key aspects covered?\n",
    "3. **Precision**: Is terminology clear?\n",
    "\n",
    "### SCORING GUIDE\n",
    "- 90-100: Excellent\n",
    "- 70-89: Good\n",
    "- 50-69: Partial\n",
    "- 0-49: Poor\n",
    "\n",
    "### OUTPUT FORMAT\n",
    "Respond ONLY with JSON:\n",
    "\n",
    "{{\n",
    "  \"score_0_100\": <integer>,\n",
    "  \"correctness\": \"<1-2 sentences>\",\n",
    "  \"completeness\": \"<1-2 sentences>\",\n",
    "  \"precision\": \"<1-2 sentences>\",\n",
    "  \"rationale\": [\"<point>\", \"<point>\", \"<point>\"]\n",
    "}}\"\"\"\n",
    "\n",
    "\n",
    "PROMPT_V3 = \"\"\"### ROLE\n",
    "You are an expert AI/ML educator.\n",
    "\n",
    "### EVALUATION PROCESS\n",
    "**Step 1:** Analyze correctness\n",
    "**Step 2:** Assess completeness\n",
    "**Step 3:** Evaluate precision\n",
    "**Step 4:** Assign score\n",
    "\n",
    "### INPUT\n",
    "**Question:** {question}\n",
    "**Target:** {target}\n",
    "**Student:** {answer}\n",
    "\n",
    "### SCORING\n",
    "- 90-100: Excellent\n",
    "- 70-89: Good\n",
    "- 50-69: Partial\n",
    "- 0-49: Poor\n",
    "\n",
    "### OUTPUT\n",
    "JSON only:\n",
    "{{\n",
    "  \"score_0_100\": <int>,\n",
    "  \"correctness\": \"<text>\",\n",
    "  \"completeness\": \"<text>\",\n",
    "  \"precision\": \"<text>\",\n",
    "  \"rationale\": [\"<point>\", \"<point>\", \"<point>\"]\n",
    "}}\"\"\"\n",
    "\n",
    "PROMPT_V4 = \"\"\"### ROLE\n",
    "You are an expert AI/ML educator evaluating student answers.\n",
    "\n",
    "### TASK\n",
    "Evaluate on:\n",
    "1. **Correctness**: Accurate concepts?\n",
    "2. **Completeness**: Key aspects covered?\n",
    "3. **Precision**: Clear terminology?\n",
    "\n",
    "---\n",
    "\n",
    "### INPUT\n",
    "**Question:** {question}\n",
    "**Target:** {target}\n",
    "**Student:** {answer}\n",
    "\n",
    "---\n",
    "\n",
    "### SCORING\n",
    "- 90-100: Excellent\n",
    "- 70-89: Good\n",
    "- 50-69: Partial\n",
    "- 0-49: Poor\n",
    "\n",
    "---\n",
    "\n",
    "### OUTPUT\n",
    "ONLY valid JSON (no markdown):\n",
    "\n",
    "{{\n",
    "  \"score_0_100\": <integer 0-100>,\n",
    "  \"correctness\": \"<1-2 sentences>\",\n",
    "  \"completeness\": \"<1-2 sentences>\",\n",
    "  \"precision\": \"<1-2 sentences>\",\n",
    "  \"rationale\": [\"<point>\", \"<point>\", \"<point>\"]\n",
    "}}\n",
    "\n",
    "**IMPORTANT:** Return ONLY the JSON object.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_student_model(model_name: str, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Wrapper function to generate student answers.\n",
    "    \"\"\"\n",
    "    if model_name.startswith(\"gpt-\"):\n",
    "        # OpenAI model\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a knowledgeable AI/ML student.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {question}\\n\\nProvide a concise answer.\"}\n",
    "        ]\n",
    "        return call_openai_model(model_name, messages)\n",
    "    else:\n",
    "        # HuggingFace model\n",
    "        prompt = f\"Question: {question}\\n\\nAnswer:\"\n",
    "        return call_huggingface_model(model_name, prompt)\n",
    "\n",
    "\n",
    "def call_evaluator(evaluator_model: str, prompt_template: str, \n",
    "                   question: str, target: str, answer: str) -> str:\n",
    "    \"\"\"\n",
    "    Wrapper function to call evaluator model.\n",
    "    \"\"\"\n",
    "    eval_prompt = prompt_template.format(\n",
    "        question=question,\n",
    "        target=target,\n",
    "        answer=answer\n",
    "    )\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert evaluator.\"},\n",
    "        {\"role\": \"user\", \"content\": eval_prompt}\n",
    "    ]\n",
    "    \n",
    "    return call_openai_model(evaluator_model, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'dict'>\n",
      "Response: {'content': None, 'latency': 0.4899141788482666, 'tokens': 0, 'cost': 0, 'success': False, 'error': '(Request ID: Root=1-690f663a-0ae7d83a116231e4213a36a9;5e172f12-a3b9-4f18-9b18-461c52dbf078)\\n\\nBad request:'}\n"
     ]
    }
   ],
   "source": [
    "# Test HuggingFace model directly\n",
    "test_response = call_student_model(\"meta-llama/Meta-Llama-3-8B-Instruct\", \"What is machine learning?\")\n",
    "print(f\"Type: {type(test_response)}\")\n",
    "print(f\"Response: {test_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 3: Test OpenAI Models (GPT-4 and GPT-4o-mini)\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL COMPARISON ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Testing 5 models on 10 questions\n",
      "Evaluator: gpt-4o\n",
      "\n",
      "\n",
      "Testing: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "----------------------------------------\n",
      "  Q1: 0\n",
      "  Q2: 0\n",
      "  Q3: 0\n",
      "  Q4: 0\n",
      "  Q5: 0\n",
      "  Q6: 0\n",
      "  Q7: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      6\u001b[39m test_models = [\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmeta-llama/Meta-Llama-3-8B-Instruct\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen2.5-7B-Instruct\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mgpt-3.5-turbo\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m ]\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Run model comparison\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m model_df, model_details = \u001b[43mcompare_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_models\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqa_db\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqa_db\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPROMPT_V4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use your best prompt\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluator_model\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4o\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcall_model_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcall_student_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcall_evaluator_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcall_evaluator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_questions\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\n\u001b[32m     24\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Visualize results\u001b[39;00m\n\u001b[32m     28\u001b[39m visualize_model_comparison(model_df, model_details)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/comparative_analysis.py:128\u001b[39m, in \u001b[36mcompare_models\u001b[39m\u001b[34m(models, qa_db, prompt_template, evaluator_model, call_model_func, call_evaluator_func, n_questions, seed)\u001b[39m\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m eval_response = \u001b[43mcall_evaluator_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluator_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqa\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqa\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43manswer\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstudent_answer\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# Parse score\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mcall_evaluator\u001b[39m\u001b[34m(evaluator_model, prompt_template, question, target, answer)\u001b[39m\n\u001b[32m     23\u001b[39m eval_prompt = prompt_template.format(\n\u001b[32m     24\u001b[39m     question=question,\n\u001b[32m     25\u001b[39m     target=target,\n\u001b[32m     26\u001b[39m     answer=answer\n\u001b[32m     27\u001b[39m )\n\u001b[32m     29\u001b[39m messages = [\n\u001b[32m     30\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mYou are an expert evaluator.\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m     31\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: eval_prompt}\n\u001b[32m     32\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_openai_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluator_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mcall_openai_model\u001b[39m\u001b[34m(model_name, messages, max_tokens, temperature)\u001b[39m\n\u001b[32m     15\u001b[39m start = time.time()\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     response = \u001b[43mopenai_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     latency = time.time() - start\n\u001b[32m     26\u001b[39m     content = response.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:1156\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1110\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1112\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1153\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1154\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1155\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/openai/_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    980\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    988\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1285\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1284\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1140\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# COMPARATIVE ANALYSIS: MODEL COMPARISON\n",
    "# ============================================================\n",
    "\n",
    "# Define models to compare\n",
    "test_models = [\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    \"google/gemma-2-9b-it\", \n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-3.5-turbo\"\n",
    "]\n",
    "\n",
    "# Run model comparison\n",
    "model_df, model_details = compare_models(\n",
    "    models=test_models,\n",
    "    qa_db=qa_db,\n",
    "    prompt_template=PROMPT_V4,  # Use your best prompt\n",
    "    evaluator_model=\"gpt-4o\",\n",
    "    call_model_func=call_student_model,\n",
    "    call_evaluator_func=call_evaluator,\n",
    "    n_questions=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "\n",
    "# Visualize results\n",
    "visualize_model_comparison(model_df, model_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPARATIVE ANALYSIS: PROMPT COMPARISON\n",
    "# ============================================================\n",
    "\n",
    "# Define all prompt versions\n",
    "prompt_versions = {\n",
    "    'PROMPT_V1': PROMPT_V1,\n",
    "    'PROMPT_V2': PROMPT_V2,\n",
    "    'PROMPT_V3': PROMPT_V3,\n",
    "    'PROMPT_V4': PROMPT_V4\n",
    "}\n",
    "\n",
    "# Select best model from previous analysis\n",
    "best_model = model_df.loc[model_df['mean_score'].idxmax(), 'model']\n",
    "print(f\"Using best model: {best_model}\")\n",
    "\n",
    "# Run prompt comparison\n",
    "prompt_df, prompt_details = compare_prompts(\n",
    "    prompt_versions=prompt_versions,\n",
    "    model_name=best_model,\n",
    "    qa_db=qa_db,\n",
    "    evaluator_model=\"gpt-4o\",\n",
    "    call_model_func=call_student_model,\n",
    "    call_evaluator_func=call_evaluator,\n",
    "    n_questions=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Visualize results\n",
    "visualize_prompt_comparison(prompt_df, prompt_details, baseline='PROMPT_V1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPARATIVE ANALYSIS: MODEL-PROMPT INTERACTION\n",
    "# ============================================================\n",
    "\n",
    "# Select top 3 models\n",
    "top_models = model_df.nlargest(3, 'mean_score')['model'].tolist()\n",
    "print(f\"Testing top models: {top_models}\")\n",
    "\n",
    "# Run interaction analysis\n",
    "interaction_df = analyze_interaction(\n",
    "    models=top_models,\n",
    "    prompt_versions=prompt_versions,\n",
    "    qa_db=qa_db,\n",
    "    evaluator_model=\"gpt-4o\",\n",
    "    call_model_func=call_student_model,\n",
    "    call_evaluator_func=call_evaluator,\n",
    "    n_questions=5,  # Fewer questions due to many combinations\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Visualize results\n",
    "visualize_interaction(interaction_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# COMPARATIVE ANALYSIS: EVIDENCE-BASED RATIONALE\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "generate_rationale(\n",
    "    model_df=model_df,\n",
    "    prompt_df=prompt_df,\n",
    "    interaction_df=interaction_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPREHENSIVE ANALYSIS (ALL-IN-ONE)\n",
    "# ============================================================\n",
    "\n",
    "models_to_test = [\"gpt-4o-mini\", \"gpt-3.5-turbo\"]\n",
    "\n",
    "prompt_versions = {\n",
    "    'PROMPT_V1': PROMPT_V1,\n",
    "    'PROMPT_V2': PROMPT_V2,\n",
    "    'PROMPT_V3': PROMPT_V3,\n",
    "    'PROMPT_V4': PROMPT_V4\n",
    "}\n",
    "\n",
    "results = run_full_analysis(\n",
    "    models=models_to_test,\n",
    "    prompt_versions=prompt_versions,\n",
    "    qa_db=qa_db,\n",
    "    evaluator_model=\"gpt-4o\",\n",
    "    call_model_func=call_student_model,\n",
    "    call_evaluator_func=call_evaluator,\n",
    "    n_questions_model=10,\n",
    "    n_questions_prompt=10,\n",
    "    n_questions_interaction=5\n",
    ")\n",
    "\n",
    "# Access results\n",
    "print(f\"\\nBest Model: {results['best_model']}\")\n",
    "print(f\"Best Prompt: {results['best_prompt']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING OPENAI MODELS\n",
      "============================================================\n",
      "\n",
      "Question: Activation Function\n",
      "Answer (truncated): An activation function is a mathematical function that transforms! each neuron’s aggregated input (p...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Testing: GPT-4\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTesting: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_config[\u001b[33m'\u001b[39m\u001b[33mdisplay\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m result = \u001b[43mcall_openai_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_messages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[33m\"\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     66\u001b[39m     score = extract_score_from_response(result[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mcall_openai_model\u001b[39m\u001b[34m(model_name, messages, max_tokens, temperature)\u001b[39m\n\u001b[32m     15\u001b[39m start = time.time()\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     response = \u001b[43mopenai_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     latency = time.time() - start\n\u001b[32m     26\u001b[39m     content = response.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:1156\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1110\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1112\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1153\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1154\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1155\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/openai/_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    980\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    988\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Designing Ai /Mini Groups Assignment/Part 2/LLM-Answer-Evaluator/.venv/lib/python3.13/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1285\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1284\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1140\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test OpenAI models first\n",
    "\"\"\"\n",
    "\n",
    "# Test case\n",
    "test_question = qa_db[0][\"question\"]\n",
    "test_target = qa_db[0][\"answer\"]\n",
    "test_answer = test_target[:200] + \" This is a simplified explanation.\"\n",
    "\n",
    "# Simple evaluation prompt for initial comparison\n",
    "SIMPLE_EVAL_PROMPT = \"\"\"You are an AI educator evaluating student answers.\n",
    "\n",
    "Question: {question}\n",
    "Target Answer: {target}\n",
    "Student Answer: {answer}\n",
    "\n",
    "Evaluate the answer and respond with JSON:\n",
    "{{\n",
    "  \"score_0_100\": 85,\n",
    "  \"correctness\": \"Brief assessment\",\n",
    "  \"completeness\": \"Brief assessment\",\n",
    "  \"precision\": \"Brief assessment\",\n",
    "  \"rationale\": [\"Point 1\", \"Point 2\", \"Point 3\"]\n",
    "}}\n",
    "\n",
    "Score: 90-100 excellent, 70-89 good, 50-69 partial, <50 poor.\"\"\"\n",
    "\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an educational AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": SIMPLE_EVAL_PROMPT.format(\n",
    "        question=test_question,\n",
    "        target=test_target,\n",
    "        answer=test_answer\n",
    "    )}\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TESTING OPENAI MODELS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nQuestion: {test_question}\")\n",
    "print(f\"Answer (truncated): {test_answer[:100]}...\\n\")\n",
    "\n",
    "openai_models = [\n",
    "    {\"name\": \"gpt-4\", \"display\": \"GPT-4\"},\n",
    "    {\"name\": \"gpt-4o-mini\", \"display\": \"GPT-4o-mini\"},\n",
    "]\n",
    "\n",
    "openai_results = []\n",
    "\n",
    "def summarize_rationale(rationale, max_words=200):\n",
    "    if isinstance(rationale, list):\n",
    "        rationale_text = \" \".join(str(point) for point in rationale)\n",
    "    else:\n",
    "        rationale_text = str(rationale)\n",
    "    words = rationale_text.split()\n",
    "    return \" \".join(words[:max_words])\n",
    "\n",
    "for model_config in openai_models:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {model_config['display']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    result = call_openai_model(model_config[\"name\"], test_messages)\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        score = extract_score_from_response(result[\"content\"])\n",
    "        \n",
    "        print(f\"✅ Success\")\n",
    "        print(f\"   Score: {score}/100\" if score else \"   Score: Could not extract\")\n",
    "        print(f\"   Latency: {result['latency']:.2f}s\")\n",
    "        print(f\"   Tokens: {result['tokens']}\")\n",
    "        print(f\"   Cost: ${result['cost']:.6f}\")\n",
    "        \n",
    "        # Show snippet\n",
    "        print(f\"\\n   Response preview:\")\n",
    "        print(f\"   {result['content'][:200]}...\\n\")\n",
    "        \n",
    "        # Extract rationale from response (assuming JSON format)\n",
    "        try:\n",
    "            response_json = json.loads(result[\"content\"])\n",
    "            rationale = response_json.get(\"rationale\", \"\")\n",
    "            rationale_summary = summarize_rationale(rationale)\n",
    "        except Exception:\n",
    "            rationale_summary = \"Could not extract rationale.\"\n",
    "        print(f\"\\n   Rationale (summary <200 words):\")\n",
    "        print(f\"   {rationale_summary}\\n\")\n",
    "        \n",
    "        openai_results.append({\n",
    "            \"name\": model_config[\"name\"],\n",
    "            \"display\": model_config[\"display\"],\n",
    "            \"model\": model_config[\"display\"],\n",
    "            \"provider\": \"openai\",\n",
    "            \"score\": score,\n",
    "            \"latency\": result[\"latency\"],\n",
    "            \"tokens\": result[\"tokens\"],\n",
    "            \"cost\": result[\"cost\"],\n",
    "            \"success\": True,\n",
    "            \"evaluation\": {\"score_0_100\": score},\n",
    "            \"full_model\": model_config[\"name\"]\n",
    "        })\n",
    "    else:\n",
    "        print(f\"❌ Failed: {result.get('error', 'Unknown error')}\")\n",
    "        openai_results.append({\n",
    "            \"name\": model_config[\"name\"],\n",
    "            \"display\": model_config[\"display\"],\n",
    "            \"model\": model_config[\"display\"],\n",
    "            \"provider\": \"openai\",\n",
    "            \"success\": False\n",
    "        })\n",
    "    \n",
    "    time.sleep(1)  # Rate limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "royalblue"
         },
         "type": "bar",
         "x": [
          "GPT-4",
          "GPT-4o-mini"
         ],
         "y": [
          65,
          70
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "OpenAI Model Scores"
        },
        "xaxis": {
         "title": {
          "text": "Model"
         }
        },
        "yaxis": {
         "range": [
          0,
          100
         ],
         "title": {
          "text": "Score (0-100)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "orange"
         },
         "type": "bar",
         "x": [
          "GPT-4",
          "GPT-4o-mini"
         ],
         "y": [
          5.381892204284668,
          5.069880962371826
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "OpenAI Model Latency"
        },
        "xaxis": {
         "title": {
          "text": "Model"
         }
        },
        "yaxis": {
         "title": {
          "text": "Latency (s)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "green"
         },
         "type": "bar",
         "x": [
          "GPT-4",
          "GPT-4o-mini"
         ],
         "y": [
          484,
          487
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "OpenAI Model Tokens Used"
        },
        "xaxis": {
         "title": {
          "text": "Model"
         }
        },
        "yaxis": {
         "title": {
          "text": "Tokens"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "red"
         },
         "type": "bar",
         "x": [
          "GPT-4",
          "GPT-4o-mini"
         ],
         "y": [
          0.01929,
          0.00014595
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "OpenAI Model Cost"
        },
        "xaxis": {
         "title": {
          "text": "Model"
         }
        },
        "yaxis": {
         "title": {
          "text": "Cost (USD)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_openai_metrics(results):\n",
    "    models = [r[\"display\"] for r in results if r.get(\"success\")]\n",
    "    scores = [r[\"score\"] for r in results if r.get(\"success\")]\n",
    "    latencies = [r[\"latency\"] for r in results if r.get(\"success\")]\n",
    "    tokens = [r[\"tokens\"] for r in results if r.get(\"success\")]\n",
    "    costs = [r[\"cost\"] for r in results if r.get(\"success\")]\n",
    "\n",
    "    # Score chart\n",
    "    fig_score = go.Figure([go.Bar(x=models, y=scores, marker_color='royalblue')])\n",
    "    fig_score.update_layout(title=\"OpenAI Model Scores\", xaxis_title=\"Model\", yaxis_title=\"Score (0-100)\", yaxis=dict(range=[0, 100]))\n",
    "    fig_score.show()\n",
    "\n",
    "    # Latency chart\n",
    "    fig_latency = go.Figure([go.Bar(x=models, y=latencies, marker_color='orange')])\n",
    "    fig_latency.update_layout(title=\"OpenAI Model Latency\", xaxis_title=\"Model\", yaxis_title=\"Latency (s)\")\n",
    "    fig_latency.show()\n",
    "\n",
    "    # Tokens chart\n",
    "    fig_tokens = go.Figure([go.Bar(x=models, y=tokens, marker_color='green')])\n",
    "    fig_tokens.update_layout(title=\"OpenAI Model Tokens Used\", xaxis_title=\"Model\", yaxis_title=\"Tokens\")\n",
    "    fig_tokens.show()\n",
    "\n",
    "    # Cost chart\n",
    "    fig_cost = go.Figure([go.Bar(x=models, y=costs, marker_color='red')])\n",
    "    fig_cost.update_layout(title=\"OpenAI Model Cost\", xaxis_title=\"Model\", yaxis_title=\"Cost (USD)\")\n",
    "    fig_cost.show()\n",
    "\n",
    "# Usage:\n",
    "plot_openai_metrics(openai_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 4: Test HuggingFace Models\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING HUGGINGFACE MODELS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Testing: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "============================================================\n",
      "✅ Success\n",
      "   Score: 85/100\n",
      "   Latency: 3.03s\n",
      "   Tokens (est): 362\n",
      "   Cost: FREE\n",
      "\n",
      "   Rationale:\n",
      "   • The student correctly identifies the main purpose and characteristics of an activation function.\n",
      "   • However, the explanation lacks some specific examples of activation functions, such as ReLU and its variants.\n",
      "   • Additionally, the student does not explicitly mention the importance of activation functions in enabling gradient-based training.\n",
      "   • The statement 'usually differentiable mapping' could be more precise, as some activation functions are not differentiable everywhere.\n",
      "\n",
      "============================================================\n",
      "Testing: Qwen/Qwen2.5-7B-Instruct\n",
      "============================================================\n",
      "✅ Success\n",
      "   Score: 75/100\n",
      "   Latency: 1.4s\n",
      "   Tokens (est): 322\n",
      "   Cost: FREE\n",
      "\n",
      "   Rationale:\n",
      "   • Lacks discussion on the role of activation functions in non-linear modeling and gradient-based training.\n",
      "   • Does not mention the desirable properties like monotonicity, bounded output range, or sparsity of activation.\n",
      "   • The ending is cut off, making the explanation incomplete.\n",
      "\n",
      "============================================================\n",
      "Testing: google/gemma-2-9b-it\n",
      "============================================================\n",
      "✅ Success\n",
      "   Score: 60/100\n",
      "   Latency: 1.38s\n",
      "   Tokens (est): 334\n",
      "   Cost: FREE\n",
      "\n",
      "   Rationale:\n",
      "   • Identifies the core function of an activation function: transforming input to output.\n",
      "   • Mentions non-linearity and differentiability, crucial for neural network learning.\n",
      "   • Fails to elaborate on the role of activation functions in backpropagation and different types (sigmoid, ReLU, etc.).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test HuggingFace models\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING HUGGINGFACE MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "hf_models = [\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    \"google/gemma-2-9b-it\"\n",
    "]\n",
    "\n",
    "def test_model_hf(model_name: str, question: str, target: str, answer: str) -> dict:\n",
    "    \"\"\"Test a HuggingFace model.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert AI/ML educator evaluating student answers.\n",
    "\n",
    "**Question:** {question}\n",
    "\n",
    "**Target Answer:** {target}\n",
    "\n",
    "**Student Answer:** {answer}\n",
    "\n",
    "Evaluate on: correctness, completeness, precision.\n",
    "\n",
    "Respond ONLY with valid JSON (no extra text):\n",
    "{{\n",
    "  \"score_0_100\": <integer>,\n",
    "  \"correctness\": \"<brief assessment>\",\n",
    "  \"completeness\": \"<brief assessment>\",\n",
    "  \"precision\": \"<brief assessment>\",\n",
    "  \"rationale\": [\"<point 1>\", \"<point 2>\", \"<point 3>\"]\n",
    "}}\"\"\"\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = hf_client.chat_completion(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=model_name,\n",
    "            max_tokens=500,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        result_text = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Clean markdown\n",
    "        if result_text.startswith(\"```\"):\n",
    "            result_text = result_text.split(\"```\")[1]\n",
    "            if result_text.startswith(\"json\"):\n",
    "                result_text = result_text[4:]\n",
    "            result_text = result_text.rsplit(\"```\", 1)[0]\n",
    "        \n",
    "        # Extract JSON\n",
    "        if \"{\" in result_text and \"}\" in result_text:\n",
    "            start = result_text.find(\"{\")\n",
    "            end = result_text.rfind(\"}\") + 1\n",
    "            result_text = result_text[start:end]\n",
    "        \n",
    "        evaluation = json.loads(result_text)\n",
    "        tokens_estimate = len(prompt.split()) * 1.3 + len(result_text.split()) * 1.3\n",
    "        \n",
    "        return {\n",
    "            \"model\": model_name.split(\"/\")[-1],\n",
    "            \"display\": model_name.split(\"/\")[-1],\n",
    "            \"full_model\": model_name,\n",
    "            \"provider\": \"huggingface\",\n",
    "            \"success\": True,\n",
    "            \"latency\": round(elapsed, 2),\n",
    "            \"evaluation\": evaluation,\n",
    "            \"score\": evaluation[\"score_0_100\"],\n",
    "            \"tokens\": int(tokens_estimate),\n",
    "            \"cost\": 0.0\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"model\": model_name.split(\"/\")[-1],\n",
    "            \"display\": model_name.split(\"/\")[-1],\n",
    "            \"full_model\": model_name,\n",
    "            \"provider\": \"huggingface\",\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "hf_results = []\n",
    "for model in hf_models:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {model}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    result = test_model_hf(model, test_question, test_target, test_answer)\n",
    "    hf_results.append(result)\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        print(f\"✅ Success\")\n",
    "        print(f\"   Score: {result['evaluation']['score_0_100']}/100\")\n",
    "        print(f\"   Latency: {result['latency']}s\")\n",
    "        print(f\"   Tokens (est): {result['tokens']}\")\n",
    "        print(f\"   Cost: FREE\")\n",
    "        print(f\"\\n   Rationale:\")\n",
    "        for point in result['evaluation']['rationale']:\n",
    "            print(f\"   • {point}\")\n",
    "    else:\n",
    "        print(f\"❌ Failed: {result['error']}\")\n",
    "        print(f\"   Note: Model may need time to load (cold start)\")\n",
    "    \n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 5: Visualize Model Comparison (All 5 Models)\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "lightblue"
         },
         "type": "bar",
         "x": [
          "GPT-4",
          "GPT-4o-mini",
          "Meta-Llama-3-8B-Instruct",
          "Qwen2.5-7B-Instruct"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "PEZVSw==",
          "dtype": "i1"
         },
         "yaxis": "y"
        },
        {
         "marker": {
          "color": "lightcoral"
         },
         "type": "bar",
         "x": [
          "GPT-4",
          "GPT-4o-mini",
          "Meta-Llama-3-8B-Instruct",
          "Qwen2.5-7B-Instruct"
         ],
         "xaxis": "x2",
         "y": {
          "bdata": "AAAAECqoF0AAAACg5ZwQQOF6FK5H4QpAmpmZmZmZ8T8=",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "marker": {
          "color": "lightgreen"
         },
         "type": "bar",
         "x": [
          "GPT-4",
          "GPT-4o-mini",
          "Meta-Llama-3-8B-Instruct",
          "Qwen2.5-7B-Instruct"
         ],
         "xaxis": "x3",
         "y": {
          "bdata": "7AHdAX4BQQE=",
          "dtype": "i2"
         },
         "yaxis": "y3"
        },
        {
         "marker": {
          "color": "lightyellow"
         },
         "type": "bar",
         "x": [
          "GPT-4",
          "GPT-4o-mini",
          "Meta-Llama-3-8B-Instruct",
          "Qwen2.5-7B-Instruct"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "rYbEPZY+lD8HUJ1Y8VciP43ttaD3xrA+je21oPfGsD4=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Evaluation Score",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Response Latency",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Token Usage",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.375,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Cost per Evaluation",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.375,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 600,
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Model Comparison: OpenAI vs HuggingFace"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.45
         ]
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.55,
          1
         ]
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0,
          0.45
         ]
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0.55,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.625,
          1
         ],
         "title": {
          "text": "Score (0-100)"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.625,
          1
         ],
         "title": {
          "text": "Seconds"
         }
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0,
          0.375
         ],
         "title": {
          "text": "Tokens"
         }
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0,
          0.375
         ],
         "title": {
          "text": "$ (log scale)"
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Model comparison visualization complete\n",
      "\n",
      "📊 Summary Table:\n",
      "                   Model  Score  Latency (s)  Tokens  Cost ($)    Provider\n",
      "                   GPT-4     60     5.914223     492   0.01977      openai\n",
      "             GPT-4o-mini     70     4.153220     477   0.00014      openai\n",
      "Meta-Llama-3-8B-Instruct     85     3.360000     382   0.00000 huggingface\n",
      "     Qwen2.5-7B-Instruct     75     1.100000     321   0.00000 huggingface\n",
      "\n",
      "============================================================\n",
      "KEY OBSERVATIONS\n",
      "============================================================\n",
      "\n",
      "**Quality (Score):**\n",
      "- Best score: Meta-Llama-3-8B-Instruct \n",
      "  (85/100)\n",
      "\n",
      "**Cost Efficiency:**\n",
      "- GPT-4: ~$0.0198 per eval (expensive)\n",
      "- GPT-4o-mini: ~$0.000140 per eval (99% cheaper!)\n",
      "- HF Models: FREE (but rate limited)\n",
      "\n",
      "**Speed:**\n",
      "- Fastest: Qwen2.5-7B-Instruct \n",
      "  (1.10s)\n",
      "- Slowest: GPT-4 \n",
      "  (5.91s)\n",
      "\n",
      "  ## write a recommendation here \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine all successful results\n",
    "all_successful = []\n",
    "all_successful.extend([r for r in openai_results if r.get(\"success\")])\n",
    "all_successful.extend([r for r in hf_results if r.get(\"success\")])\n",
    "\n",
    "if len(all_successful) == 0:\n",
    "    print(\"⚠️ No successful results to visualize\")\n",
    "    print(\"   Models may be loading. Wait 30s and try again.\")\n",
    "else:\n",
    "    model_comparison_df = pd.DataFrame([\n",
    "        {\n",
    "            \"Model\": r[\"model\"],\n",
    "            \"Score\": r.get(\"score\", r[\"evaluation\"][\"score_0_100\"]),\n",
    "            \"Latency (s)\": r[\"latency\"],\n",
    "            \"Tokens\": r[\"tokens\"],\n",
    "            \"Cost ($)\": r[\"cost\"],\n",
    "            \"Provider\": r[\"provider\"]\n",
    "        }\n",
    "        for r in all_successful\n",
    "    ])\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\"Evaluation Score\", \"Response Latency\", \n",
    "                       \"Token Usage\", \"Cost per Evaluation\"),\n",
    "    )\n",
    "    \n",
    "    # Score comparison\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=model_comparison_df[\"Model\"], y=model_comparison_df[\"Score\"],\n",
    "               marker_color=\"lightblue\"),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Latency\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=model_comparison_df[\"Model\"], y=model_comparison_df[\"Latency (s)\"],\n",
    "               marker_color=\"lightcoral\"),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Tokens\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=model_comparison_df[\"Model\"], y=model_comparison_df[\"Tokens\"],\n",
    "               marker_color=\"lightgreen\"),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Cost (with small offset for free models)\n",
    "    costs_display = model_comparison_df[\"Cost ($)\"].replace(0, 0.000001)  # For log scale\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=model_comparison_df[\"Model\"], y=costs_display,\n",
    "               marker_color=\"lightyellow\"),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=600, \n",
    "        showlegend=False, \n",
    "        title_text=\"Model Comparison: OpenAI vs HuggingFace\"\n",
    "    )\n",
    "    fig.update_yaxes(title_text=\"Score (0-100)\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Seconds\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Tokens\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"$ (log scale)\", type=\"log\", row=2, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    print(\"\\n✅ Model comparison visualization complete\")\n",
    "    print(\"\\n📊 Summary Table:\")\n",
    "    print(model_comparison_df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\"\"\n",
    "============================================================\n",
    "KEY OBSERVATIONS\n",
    "============================================================\n",
    "\n",
    "**Quality (Score):**\n",
    "- Best score: {model_comparison_df.loc[model_comparison_df['Score'].idxmax(), 'Model']} \n",
    "  ({model_comparison_df['Score'].max()}/100)\n",
    "\n",
    "**Cost Efficiency:**\n",
    "- GPT-4: ~${model_comparison_df[model_comparison_df['Model']=='GPT-4']['Cost ($)'].values[0] if 'GPT-4' in model_comparison_df['Model'].values else 'N/A':.4f} per eval (expensive)\n",
    "- GPT-4o-mini: ~${model_comparison_df[model_comparison_df['Model']=='GPT-4o-mini']['Cost ($)'].values[0] if 'GPT-4o-mini' in model_comparison_df['Model'].values else 'N/A':.6f} per eval (99% cheaper!)\n",
    "- HF Models: FREE (but rate limited)\n",
    "\n",
    "**Speed:**\n",
    "- Fastest: {model_comparison_df.loc[model_comparison_df['Latency (s)'].idxmin(), 'Model']} \n",
    "  ({model_comparison_df['Latency (s)'].min():.2f}s)\n",
    "- Slowest: {model_comparison_df.loc[model_comparison_df['Latency (s)'].idxmax(), 'Model']} \n",
    "  ({model_comparison_df['Latency (s)'].max():.2f}s)\n",
    "\n",
    "  ## write a recommendation here \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 6: Model Selection Decision\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SELECTED MODEL: Qwen2.5-7B-Instruct\n",
      "============================================================\n",
      "\n",
      "Full model ID: Qwen/Qwen2.5-7B-Instruct\n",
      "Provider: huggingface\n",
      "\n",
      "Rationale:\n",
      "✅ Composite Score: 0.875\n",
      "✅ Quality Score: 75/100\n",
      "✅ Latency: 1.10s\n",
      "✅ Cost: FREE per evaluation\n",
      "✅ Best overall value for educational Q&A\n",
      "\n",
      "Scoring Breakdown:\n",
      "• Quality (50% weight): 75/100\n",
      "• Cost efficiency (30% weight): FREE\n",
      "• Speed (20% weight): 1.10s\n",
      "\n",
      "Alternative models tested:\n",
      "\n",
      "  1. Meta-Llama-3-8B-Instruct:\n",
      "     Composite: 0.844 | Score: 85/100 | Latency: 3.36s | Cost: FREE\n",
      "  2. GPT-4o-mini:\n",
      "     Composite: 0.746 | Score: 70/100 | Latency: 4.15s | Cost: $0.000140\n",
      "  3. GPT-4:\n",
      "     Composite: 0.383 | Score: 60/100 | Latency: 5.91s | Cost: $0.019770\n",
      "\n",
      "💡 Decision: Using Qwen2.5-7B-Instruct for prompt engineering\n",
      "\n",
      "✅ Selected: Qwen/Qwen2.5-7B-Instruct\n",
      "✅ Provider: huggingface\n"
     ]
    }
   ],
   "source": [
    "# Decision matrix: prioritize quality, then cost, then speed\n",
    "if len(all_successful) > 0:\n",
    "    # Calculate composite score for each model\n",
    "    for result in all_successful:\n",
    "        # Normalize metrics\n",
    "        score_norm = result.get(\"score\", result[\"evaluation\"][\"score_0_100\"]) / 100\n",
    "        \n",
    "        # Cost score (lower cost = better)\n",
    "        cost = result[\"cost\"]\n",
    "        if cost == 0:\n",
    "            cost_score = 1.0  # Free = perfect\n",
    "        else:\n",
    "            cost_score = min(1.0, 0.001 / cost)  # $0.001 as reference\n",
    "        \n",
    "        # Speed score (lower latency = better)\n",
    "        latency = result[\"latency\"]\n",
    "        latency_score = min(1.0, 2.0 / latency)  # 2s as reference\n",
    "        \n",
    "        # Weighted: 50% quality, 30% cost, 20% speed\n",
    "        result[\"composite_score\"] = (\n",
    "            0.5 * score_norm +\n",
    "            0.3 * cost_score +\n",
    "            0.2 * latency_score\n",
    "        )\n",
    "    \n",
    "    # Select best\n",
    "    best_result = max(all_successful, key=lambda x: x[\"composite_score\"])\n",
    "    \n",
    "    SELECTED_MODEL = best_result[\"full_model\"]\n",
    "    SELECTED_PROVIDER = best_result[\"provider\"]\n",
    "    selected_short_name = best_result[\"model\"]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"SELECTED MODEL: {selected_short_name}\")\n",
    "    print(\"=\"*60)\n",
    "    score_val = best_result.get(\"score\", best_result[\"evaluation\"][\"score_0_100\"])\n",
    "    cost_val = best_result[\"cost\"]\n",
    "    cost_str = \"FREE\" if cost_val == 0 else f\"${cost_val:.6f}\"\n",
    "    \n",
    "    print(f\"\"\"\n",
    "Full model ID: {SELECTED_MODEL}\n",
    "Provider: {SELECTED_PROVIDER}\n",
    "\n",
    "Rationale:\n",
    "✅ Composite Score: {best_result['composite_score']:.3f}\n",
    "✅ Quality Score: {score_val}/100\n",
    "✅ Latency: {best_result['latency']:.2f}s\n",
    "✅ Cost: {cost_str} per evaluation\n",
    "✅ Best overall value for educational Q&A\n",
    "\n",
    "Scoring Breakdown:\n",
    "• Quality (50% weight): {score_val}/100\n",
    "• Cost efficiency (30% weight): {cost_str}\n",
    "• Speed (20% weight): {best_result['latency']:.2f}s\n",
    "\n",
    "Alternative models tested:\n",
    "\"\"\")\n",
    "    \n",
    "    # Show alternatives\n",
    "    others = sorted(\n",
    "        [r for r in all_successful if r != best_result],\n",
    "        key=lambda x: -x[\"composite_score\"]\n",
    "    )\n",
    "    \n",
    "    for i, r in enumerate(others, 1):\n",
    "        r_score = r.get(\"score\", r[\"evaluation\"][\"score_0_100\"])\n",
    "        r_cost = r[\"cost\"]\n",
    "        r_cost_str = \"FREE\" if r_cost == 0 else f\"${r_cost:.6f}\"\n",
    "        print(f\"  {i}. {r['model']}:\")\n",
    "        print(f\"     Composite: {r['composite_score']:.3f} | \"\n",
    "              f\"Score: {r_score}/100 | \"\n",
    "              f\"Latency: {r['latency']:.2f}s | \"\n",
    "              f\"Cost: {r_cost_str}\")\n",
    "    \n",
    "    print(f\"\\n💡 Decision: Using {selected_short_name} for prompt engineering\")\n",
    "    \n",
    "else:\n",
    "    # Fallback\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"⚠️ Using Default Model\")\n",
    "    print(\"=\"*60)\n",
    "    SELECTED_MODEL = \"gpt-4o-mini\"\n",
    "    SELECTED_PROVIDER = \"openai\"\n",
    "    selected_short_name = \"GPT-4o-mini\"\n",
    "    print(\"\\nDEFAULT: GPT-4o-mini (best balance)\")\n",
    "\n",
    "print(f\"\\n✅ Selected: {SELECTED_MODEL}\")\n",
    "print(f\"✅ Provider: {SELECTED_PROVIDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 7: Prompt Engineering - Version 1 (Baseline)\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PROMPT V1: Baseline\n",
      "============================================================\n",
      "Using: Qwen2.5-7B-Instruct\n",
      "\n",
      "✅ Score: 75/100\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Prompt Engineering: Testing different prompt formulations\n",
    "Version 1: Minimal baseline\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_V1 = \"\"\"Evaluate this student answer.\n",
    "\n",
    "Question: {question}\n",
    "Target: {target}\n",
    "Answer: {answer}\n",
    "\n",
    "Score 0-100 and explain. Return JSON with score_0_100, correctness, completeness, precision, rationale.\"\"\"\n",
    "\n",
    "def test_prompt_unified(prompt_template: str, question: str, target: str, answer: str, version: str) -> dict:\n",
    "    \"\"\"Test a prompt with the selected model.\"\"\"\n",
    "    prompt = prompt_template.format(question=question, target=target, answer=answer)\n",
    "    \n",
    "    try:\n",
    "        if SELECTED_PROVIDER == \"openai\":\n",
    "            response = openai_client.chat.completions.create(\n",
    "                model=SELECTED_MODEL,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=500,\n",
    "                temperature=0.3\n",
    "            )\n",
    "            result_text = response.choices[0].message.content.strip()\n",
    "        else:\n",
    "            response = hf_client.chat_completion(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=SELECTED_MODEL,\n",
    "                max_tokens=500,\n",
    "                temperature=0.3\n",
    "            )\n",
    "            result_text = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Clean markdown\n",
    "        if result_text.startswith(\"```\"):\n",
    "            result_text = result_text.split(\"```\")[1]\n",
    "            if result_text.startswith(\"json\"):\n",
    "                result_text = result_text[4:]\n",
    "            result_text = result_text.rsplit(\"```\", 1)[0]\n",
    "        \n",
    "        # Extract JSON\n",
    "        if \"{\" in result_text and \"}\" in result_text:\n",
    "            start = result_text.find(\"{\")\n",
    "            end = result_text.rfind(\"}\") + 1\n",
    "            result_text = result_text[start:end]\n",
    "        \n",
    "        evaluation = json.loads(result_text)\n",
    "        \n",
    "        return {\n",
    "            \"version\": version,\n",
    "            \"success\": True,\n",
    "            \"score\": evaluation.get(\"score_0_100\"),\n",
    "            \"evaluation\": evaluation\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"version\": version,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PROMPT V1: Baseline\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Using: {selected_short_name}\\n\")\n",
    "\n",
    "result_v1 = test_prompt_unified(PROMPT_V1, test_question, test_target, test_answer, \"V1\")\n",
    "if result_v1[\"success\"]:\n",
    "    print(f\"✅ Score: {result_v1['score']}/100\")\n",
    "else:\n",
    "    print(f\"❌ Failed: {result_v1['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 8: Prompt Engineering - Version 2 (Structured)\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROMPT V2: Structured\n",
      "============================================================\n",
      "✅ Score: 75/100\n",
      "   The student's answer lacks key details about the activation function's role in gradient-based training and its impact on the stability of learning and representational power.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Version 2: Structured with best practices\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_V2 = \"\"\"### ROLE\n",
    "You are an expert AI/ML educator evaluating student answers.\n",
    "\n",
    "### TASK\n",
    "Evaluate the student's answer by comparing it to the target.\n",
    "\n",
    "### INPUT DATA\n",
    "**Question:** {question}\n",
    "\n",
    "**Target Answer:** {target}\n",
    "\n",
    "**Student Answer:** {answer}\n",
    "\n",
    "### EVALUATION CRITERIA\n",
    "1. **Correctness**: Are core concepts accurate?\n",
    "2. **Completeness**: Are key aspects covered?\n",
    "3. **Precision**: Is terminology clear?\n",
    "\n",
    "### SCORING GUIDE\n",
    "- 90-100: Excellent\n",
    "- 70-89: Good\n",
    "- 50-69: Partial\n",
    "- 0-49: Poor\n",
    "\n",
    "### OUTPUT FORMAT\n",
    "Respond ONLY with JSON:\n",
    "\n",
    "{{\n",
    "  \"score_0_100\": <integer>,\n",
    "  \"correctness\": \"<1-2 sentences>\",\n",
    "  \"completeness\": \"<1-2 sentences>\",\n",
    "  \"precision\": \"<1-2 sentences>\",\n",
    "  \"rationale\": [\"<point>\", \"<point>\", \"<point>\"]\n",
    "}}\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROMPT V2: Structured\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result_v2 = test_prompt_unified(PROMPT_V2, test_question, test_target, test_answer, \"V2\")\n",
    "if result_v2[\"success\"]:\n",
    "    print(f\"✅ Score: {result_v2['score']}/100\")\n",
    "    print(f\"   {result_v2['evaluation']['rationale'][0]}\")\n",
    "else:\n",
    "    print(f\"❌ Failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 9: Prompt Engineering - Version 3 (Chain of Thought)\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROMPT V3: Chain of Thought\n",
      "============================================================\n",
      "✅ Score: 75/100\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Version 3: Chain of thought reasoning\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_V3 = \"\"\"### ROLE\n",
    "You are an expert AI/ML educator.\n",
    "\n",
    "### EVALUATION PROCESS\n",
    "**Step 1:** Analyze correctness\n",
    "**Step 2:** Assess completeness\n",
    "**Step 3:** Evaluate precision\n",
    "**Step 4:** Assign score\n",
    "\n",
    "### INPUT\n",
    "**Question:** {question}\n",
    "**Target:** {target}\n",
    "**Student:** {answer}\n",
    "\n",
    "### SCORING\n",
    "- 90-100: Excellent\n",
    "- 70-89: Good\n",
    "- 50-69: Partial\n",
    "- 0-49: Poor\n",
    "\n",
    "### OUTPUT\n",
    "JSON only:\n",
    "{{\n",
    "  \"score_0_100\": <int>,\n",
    "  \"correctness\": \"<text>\",\n",
    "  \"completeness\": \"<text>\",\n",
    "  \"precision\": \"<text>\",\n",
    "  \"rationale\": [\"<point>\", \"<point>\", \"<point>\"]\n",
    "}}\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROMPT V3: Chain of Thought\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result_v3 = test_prompt_unified(PROMPT_V3, test_question, test_target, test_answer, \"V3\")\n",
    "if result_v3[\"success\"]:\n",
    "    print(f\"✅ Score: {result_v3['score']}/100\")\n",
    "else:\n",
    "    print(f\"❌ Failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 10: Prompt Engineering - Version 4 (Optimized)\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROMPT V4: Optimized\n",
      "============================================================\n",
      "✅ Score: 75/100\n",
      "   Full evaluation:\n",
      "   - Correctness: The student's answer is mostly correct, but it lacks the depth and detail provided in the target answer.\n",
      "   - Completeness: The student covers the key aspects of an activation function, but the explanation is simplified and does not include all the nuances mentioned in the target answer.\n",
      "   - Precision: The terminology used is mostly precise, but the student could provide more specific examples of activation functions and their properties.\n",
      "\n",
      "============================================================\n",
      "PROMPT VERSION SUMMARY\n",
      "============================================================\n",
      "\n",
      "📊 Scores by Version:\n",
      "  ✅ V1: 75/100\n",
      "  ✅ V2: 75/100\n",
      "  ✅ V3: 75/100\n",
      "  ✅ V4: 75/100\n",
      "\n",
      "🏆 Best: V1 (75/100)\n",
      "\n",
      "Recommendation: Use PROMPT_V1 for production\n",
      "\n",
      "✅ Prompt engineering complete\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Version 4: Optimized final version\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_V4 = \"\"\"### ROLE\n",
    "You are an expert AI/ML educator evaluating student answers.\n",
    "\n",
    "### TASK\n",
    "Evaluate on:\n",
    "1. **Correctness**: Accurate concepts?\n",
    "2. **Completeness**: Key aspects covered?\n",
    "3. **Precision**: Clear terminology?\n",
    "\n",
    "---\n",
    "\n",
    "### INPUT\n",
    "**Question:** {question}\n",
    "**Target:** {target}\n",
    "**Student:** {answer}\n",
    "\n",
    "---\n",
    "\n",
    "### SCORING\n",
    "- 90-100: Excellent\n",
    "- 70-89: Good\n",
    "- 50-69: Partial\n",
    "- 0-49: Poor\n",
    "\n",
    "---\n",
    "\n",
    "### OUTPUT\n",
    "ONLY valid JSON (no markdown):\n",
    "\n",
    "{{\n",
    "  \"score_0_100\": <integer 0-100>,\n",
    "  \"correctness\": \"<1-2 sentences>\",\n",
    "  \"completeness\": \"<1-2 sentences>\",\n",
    "  \"precision\": \"<1-2 sentences>\",\n",
    "  \"rationale\": [\"<point>\", \"<point>\", \"<point>\"]\n",
    "}}\n",
    "\n",
    "**IMPORTANT:** Return ONLY the JSON object.\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROMPT V4: Optimized\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result_v4 = test_prompt_unified(PROMPT_V4, test_question, test_target, test_answer, \"V4\")\n",
    "if result_v4[\"success\"]:\n",
    "    print(f\"✅ Score: {result_v4['score']}/100\")\n",
    "    print(f\"   Full evaluation:\")\n",
    "    print(f\"   - Correctness: {result_v4['evaluation']['correctness']}\")\n",
    "    print(f\"   - Completeness: {result_v4['evaluation']['completeness']}\")\n",
    "    print(f\"   - Precision: {result_v4['evaluation']['precision']}\")\n",
    "else:\n",
    "    print(f\"❌ Failed\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROMPT VERSION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "prompt_results = [result_v1, result_v2, result_v3, result_v4]\n",
    "successful_prompts = [r for r in prompt_results if r[\"success\"]]\n",
    "\n",
    "if successful_prompts:\n",
    "    print(\"\\n📊 Scores by Version:\")\n",
    "    for r in prompt_results:\n",
    "        status = \"✅\" if r[\"success\"] else \"❌\"\n",
    "        score = f\"{r['score']}/100\" if r[\"success\"] else \"FAILED\"\n",
    "        print(f\"  {status} {r['version']}: {score}\")\n",
    "    \n",
    "    best_prompt = max(successful_prompts, key=lambda x: x[\"score\"])\n",
    "    print(f\"\\n🏆 Best: {best_prompt['version']} ({best_prompt['score']}/100)\")\n",
    "    print(f\"\\nRecommendation: Use PROMPT_{best_prompt['version']} for production\")\n",
    "else:\n",
    "    print(\"\\n⚠️ All prompts failed\")\n",
    "\n",
    "print(\"\\n✅ Prompt engineering complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 11: FIXED - Comprehensive Prompt Testing\n",
    "# ============================================================\n",
    "# This cell ACTUALLY tests prompts with different answer qualities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPREHENSIVE PROMPT TESTING\n",
      "============================================================\n",
      "\n",
      "Using: Qwen2.5-7B-Instruct\n",
      "\n",
      "\n",
      "============================================================\n",
      "Testing Prompt V1\n",
      "============================================================\n",
      "\n",
      "  Test: Perfect\n",
      "    ⚠️ Score: 85/100 (expected: 90-100)\n",
      "  Test: Good\n",
      "    ❌ Failed\n",
      "  Test: Partial\n",
      "    ❌ Failed\n",
      "  Test: Wrong\n",
      "    ❌ Failed\n",
      "\n",
      "============================================================\n",
      "Testing Prompt V2\n",
      "============================================================\n",
      "\n",
      "  Test: Perfect\n",
      "    ❌ Failed\n",
      "  Test: Good\n",
      "    ❌ Failed\n",
      "  Test: Partial\n",
      "    ❌ Failed\n",
      "  Test: Wrong\n",
      "    ❌ Failed\n",
      "\n",
      "============================================================\n",
      "Testing Prompt V3\n",
      "============================================================\n",
      "\n",
      "  Test: Perfect\n",
      "    ❌ Failed\n",
      "  Test: Good\n",
      "    ❌ Failed\n",
      "  Test: Partial\n",
      "    ❌ Failed\n",
      "  Test: Wrong\n",
      "    ❌ Failed\n",
      "\n",
      "============================================================\n",
      "Testing Prompt V4\n",
      "============================================================\n",
      "\n",
      "  Test: Perfect\n",
      "    ❌ Failed\n",
      "  Test: Good\n",
      "    ❌ Failed\n",
      "  Test: Partial\n",
      "    ❌ Failed\n",
      "  Test: Wrong\n",
      "    ❌ Failed\n",
      "\n",
      "============================================================\n",
      "RESULTS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Calibration Accuracy:\n",
      "        in_range\n",
      "prompt          \n",
      "V1      0/1 (0%)\n",
      "\n",
      "INTERPRETATION:\n",
      "- Higher accuracy = better calibration\n",
      "- Look for balance between accuracy and complexity\n",
      "\n",
      "RECOMMENDATION:\n",
      "Choose the prompt with highest accuracy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"COMPREHENSIVE PROMPT TESTING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nUsing: {selected_short_name}\\n\")\n",
    "\n",
    "# Test cases with different quality levels\n",
    "test_cases = [\n",
    "    {\n",
    "        \"name\": \"Perfect\",\n",
    "        \"question\": \"Activation Function\",\n",
    "        \"target\": qa_db[0][\"answer\"],\n",
    "        \"answer\": qa_db[0][\"answer\"],\n",
    "        \"expected\": (90, 100)\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Good\",\n",
    "        \"question\": \"Activation Function\",\n",
    "        \"target\": qa_db[0][\"answer\"],\n",
    "        \"answer\": \"\"\"An activation function applies non-linear transformation to neuron inputs, \n",
    "        allowing neural networks to learn complex patterns. Common examples include sigmoid, tanh, \n",
    "        and ReLU. Without activation functions, neural networks would be linear models.\"\"\",\n",
    "        \"expected\": (75, 89)\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Partial\",\n",
    "        \"question\": \"Activation Function\",\n",
    "        \"target\": qa_db[0][\"answer\"],\n",
    "        \"answer\": \"Activation functions are used in neural networks. They help with learning.\",\n",
    "        \"expected\": (40, 60)\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Wrong\",\n",
    "        \"question\": \"Activation Function\",\n",
    "        \"target\": qa_db[0][\"answer\"],\n",
    "        \"answer\": \"An activation function is a gradient descent optimizer.\",\n",
    "        \"expected\": (0, 30)\n",
    "    }\n",
    "]\n",
    "\n",
    "prompts_to_test = [\n",
    "    (\"V1\", PROMPT_V1),\n",
    "    (\"V2\", PROMPT_V2),\n",
    "    (\"V3\", PROMPT_V3),\n",
    "    (\"V4\", PROMPT_V4)\n",
    "]\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for prompt_name, prompt_template in prompts_to_test:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing Prompt {prompt_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for test_case in test_cases:\n",
    "        print(f\"  Test: {test_case['name']}\")\n",
    "        \n",
    "        result = test_prompt_unified(\n",
    "            prompt_template,\n",
    "            test_case[\"question\"],\n",
    "            test_case[\"target\"],\n",
    "            test_case[\"answer\"],\n",
    "            prompt_name\n",
    "        )\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            score = result[\"score\"]\n",
    "            exp_min, exp_max = test_case[\"expected\"]\n",
    "            in_range = exp_min <= score <= exp_max\n",
    "            status = \"✅\" if in_range else \"⚠️\"\n",
    "            \n",
    "            print(f\"    {status} Score: {score}/100 (expected: {exp_min}-{exp_max})\")\n",
    "            \n",
    "            test_results.append({\n",
    "                \"prompt\": prompt_name,\n",
    "                \"test\": test_case[\"name\"],\n",
    "                \"score\": score,\n",
    "                \"expected_min\": exp_min,\n",
    "                \"expected_max\": exp_max,\n",
    "                \"in_range\": in_range\n",
    "            })\n",
    "        else:\n",
    "            print(f\"    ❌ Failed\")\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Analysis\n",
    "df_test = pd.DataFrame(test_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(df_test) > 0:\n",
    "    accuracy = df_test.groupby('prompt').agg({\n",
    "        'in_range': lambda x: f\"{x.sum()}/{len(x)} ({x.mean()*100:.0f}%)\"\n",
    "    })\n",
    "    \n",
    "    print(\"\\nCalibration Accuracy:\")\n",
    "    print(accuracy)\n",
    "    \n",
    "    print(\"\"\"\n",
    "INTERPRETATION:\n",
    "- Higher accuracy = better calibration\n",
    "- Look for balance between accuracy and complexity\n",
    "\n",
    "RECOMMENDATION:\n",
    "Choose the prompt with highest accuracy.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 12: EXPLAINED - Consistency Testing\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CONSISTENCY TESTING\n",
      "============================================================\n",
      "\n",
      "Testing: Same answer, multiple evaluations\n",
      "\n",
      "Running 5 evaluations...\n",
      "\n",
      "  Trial 1: 75/100\n",
      "  Trial 2: 75/100\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## CONSISTENCY TESTING EXPLAINED\n",
    "\n",
    "**What:** Do we get similar scores evaluating the SAME answer multiple times?\n",
    "**Why:** LLMs are stochastic (random). We need to verify reliability.\n",
    "**How:** Evaluate one answer 5 times, calculate variance.\n",
    "**Good:** Variance ≤5 points (comparable to human graders)\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CONSISTENCY TESTING\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nTesting: Same answer, multiple evaluations\\n\")\n",
    "\n",
    "# Test answer\n",
    "test_ans = \"\"\"An activation function introduces non-linearity into neural networks by \n",
    "transforming the weighted sum of inputs. Common functions include ReLU, sigmoid, and tanh. \n",
    "Without activation functions, stacking layers would still result in linear transformation.\"\"\"\n",
    "\n",
    "# Use best prompt\n",
    "BEST_PROMPT = PROMPT_V4\n",
    "N_REPS = 5\n",
    "\n",
    "print(f\"Running {N_REPS} evaluations...\\n\")\n",
    "\n",
    "consistency_scores = []\n",
    "\n",
    "for i in range(N_REPS):\n",
    "    result = test_prompt_unified(\n",
    "        BEST_PROMPT,\n",
    "        \"Activation Function\",\n",
    "        qa_db[0][\"answer\"],\n",
    "        test_ans,\n",
    "        f\"consistency_{i}\"\n",
    "    )\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        score = result[\"score\"]\n",
    "        consistency_scores.append(score)\n",
    "        print(f\"  Trial {i+1}: {score}/100\")\n",
    "    time.sleep(0.5)\n",
    "\n",
    "if len(consistency_scores) >= 3:\n",
    "    mean_score = np.mean(consistency_scores)\n",
    "    std_score = np.std(consistency_scores)\n",
    "    min_score = np.min(consistency_scores)\n",
    "    max_score = np.max(consistency_scores)\n",
    "    score_range = max_score - min_score\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CONSISTENCY RESULTS\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    print(f\"Scores: {consistency_scores}\")\n",
    "    print(f\"\\nMean: {mean_score:.1f}/100\")\n",
    "    print(f\"Std Dev: {std_score:.2f}\")\n",
    "    print(f\"Range: {min_score}-{max_score} ({score_range} points)\\n\")\n",
    "    \n",
    "    if score_range <= 5:\n",
    "        assessment = \"✅ EXCELLENT - Highly consistent\"\n",
    "    elif score_range <= 10:\n",
    "        assessment = \"✅ GOOD - Acceptable consistency\"\n",
    "    else:\n",
    "        assessment = \"⚠️ NEEDS IMPROVEMENT\"\n",
    "    \n",
    "    print(f\"Assessment: {assessment}\\n\")\n",
    "    \n",
    "    print(\"\"\"\n",
    "WHAT THIS MEANS:\n",
    "- Consistency measures RELIABILITY (precision)\n",
    "- Same input → similar output\n",
    "- Comparable to human inter-rater reliability\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 13: EXPLAINED - Calibration Testing\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CALIBRATION TESTING\n",
      "============================================================\n",
      "\n",
      "Testing: Do scores match quality levels?\n",
      "\n",
      "Test 1: Excellent answer\n",
      "Test 2: Good answer\n",
      "Test 3: Satisfactory answer\n",
      "Test 4: Poor answer\n",
      "============================================================\n",
      "CALIBRATION RESULTS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## CALIBRATION TESTING EXPLAINED\n",
    "\n",
    "**What:** Do scores match actual answer quality?\n",
    "**Why:** An LLM can be consistent but wrong!\n",
    "**How:** Test answers of known quality, check if scores align.\n",
    "**Good:** 90%+ accuracy (scores in expected ranges)\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CALIBRATION TESTING\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nTesting: Do scores match quality levels?\\n\")\n",
    "\n",
    "# Calibration test set with known quality\n",
    "calibration_tests = [\n",
    "    # Excellent (90-100)\n",
    "    {\n",
    "        \"category\": \"Excellent\",\n",
    "        \"q\": \"Activation Function\",\n",
    "        \"target\": qa_db[0][\"answer\"],\n",
    "        \"answer\": qa_db[0][\"answer\"],\n",
    "        \"expected\": (90, 100)\n",
    "    },\n",
    "    # Good (75-89)\n",
    "    {\n",
    "        \"category\": \"Good\",\n",
    "        \"q\": \"Activation Function\",\n",
    "        \"target\": qa_db[0][\"answer\"],\n",
    "        \"answer\": \"\"\"Activation functions apply non-linear transformations to neuron inputs. \n",
    "        Examples include ReLU, sigmoid, tanh. They're essential because without them, \n",
    "        multiple layers would collapse into linear transformation.\"\"\",\n",
    "        \"expected\": (75, 89)\n",
    "    },\n",
    "    # Satisfactory (60-74)\n",
    "    {\n",
    "        \"category\": \"Satisfactory\",\n",
    "        \"q\": \"Activation Function\",\n",
    "        \"target\": qa_db[0][\"answer\"],\n",
    "        \"answer\": \"Activation functions are mathematical functions in neural networks. \"\n",
    "                 \"They help introduce non-linearity. Examples are sigmoid and ReLU.\",\n",
    "        \"expected\": (60, 74)\n",
    "    },\n",
    "    # Poor (0-50)\n",
    "    {\n",
    "        \"category\": \"Poor\",\n",
    "        \"q\": \"Activation Function\",\n",
    "        \"target\": qa_db[0][\"answer\"],\n",
    "        \"answer\": \"Activation functions activate the neurons.\",\n",
    "        \"expected\": (0, 50)\n",
    "    },\n",
    "]\n",
    "\n",
    "calibration_results = []\n",
    "\n",
    "for i, test in enumerate(calibration_tests, 1):\n",
    "    print(f\"Test {i}: {test['category']} answer\")\n",
    "    \n",
    "    result = test_prompt_unified(\n",
    "        BEST_PROMPT,\n",
    "        test[\"q\"],\n",
    "        test[\"target\"],\n",
    "        test[\"answer\"],\n",
    "        f\"cal_{i}\"\n",
    "    )\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        score = result[\"score\"]\n",
    "        exp_min, exp_max = test[\"expected\"]\n",
    "        in_range = exp_min <= score <= exp_max\n",
    "        status = \"✅\" if in_range else \"❌\"\n",
    "        \n",
    "        print(f\"  {status} Score: {score}/100 (expected: {exp_min}-{exp_max})\\n\")\n",
    "        \n",
    "        calibration_results.append({\n",
    "            \"category\": test[\"category\"],\n",
    "            \"score\": score,\n",
    "            \"expected_min\": exp_min,\n",
    "            \"expected_max\": exp_max,\n",
    "            \"in_range\": in_range\n",
    "        })\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "\n",
    "df_cal = pd.DataFrame(calibration_results)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CALIBRATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(df_cal) > 0:\n",
    "    accuracy = df_cal['in_range'].mean() * 100\n",
    "    print(f\"\\n✅ Calibration Accuracy: {accuracy:.0f}%\")\n",
    "    print(f\"   ({df_cal['in_range'].sum()}/{len(df_cal)} in range)\\n\")\n",
    "    \n",
    "    if accuracy >= 90:\n",
    "        assessment = \"✅ EXCELLENT - Scores align with standards\"\n",
    "    elif accuracy >= 75:\n",
    "        assessment = \"✅ GOOD - Acceptable calibration\"\n",
    "    else:\n",
    "        assessment = \"⚠️ NEEDS IMPROVEMENT\"\n",
    "    \n",
    "    print(f\"{assessment}\\n\")\n",
    "    \n",
    "    print(\"\"\"\n",
    "WHAT CALIBRATION TELLS US:\n",
    "- Calibration measures VALIDITY (accuracy)\n",
    "- Quality level → appropriate score\n",
    "- Different from consistency!\n",
    "\n",
    "THE DIFFERENCE:\n",
    "CONSISTENCY: Same answer → same score (reliability)\n",
    "CALIBRATION: Score matches quality (accuracy)\n",
    "\n",
    "NEED BOTH for trustworthy evaluation! ✅\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ MODEL BUILD COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "FINAL CONFIGURATION:\n",
    "====================\n",
    "Selected Model: {selected_short_name}\n",
    "Provider: {SELECTED_PROVIDER}\n",
    "Selected Prompt: {best_prompt['version'] if 'best_prompt' in locals() and best_prompt else 'V4'}\n",
    "\n",
    "VALIDATION RESULTS:\n",
    "===================\n",
    "✅ Model comparison: Tested 5 models\n",
    "✅ Prompt engineering: Tested 4 versions\n",
    "✅ Consistency: Tested reliability\n",
    "✅ Calibration: Tested validity\n",
    "\n",
    "NEXT STEPS:\n",
    "===========\n",
    "1. Copy configuration to model_run.ipynb\n",
    "2. Implement in model_app.py\n",
    "3. Deploy to Streamlit\n",
    "4. Monitor performance with real users\n",
    "\n",
    "Ready for production! 🚀\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
