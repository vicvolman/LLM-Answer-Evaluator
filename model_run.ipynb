{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A Evaluator - Core Implementation\n",
    "Assignment 11.02 - LLM Applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Libraries\n",
    "Run this cell first to install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# !pip install rouge-score openai python-dotenv\n",
    "\n",
    "import json\n",
    "import uuid\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "# ROUGE metrics\n",
    "try:\n",
    "    from rouge_score import rouge_scorer\n",
    "    ROUGE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    ROUGE_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è rouge_score not installed. Run: pip install rouge-score\")\n",
    "\n",
    "# LLM client (OpenAI example - adjust for your provider)\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    LLM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LLM_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è openai not installed. Run: pip install openai\")\n",
    "\n",
    "print(\"‚úÖ Imports loaded successfully\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 1. LOAD Q&A DATABASE\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 150 questions\n",
      "Sample question: Activation Function\n"
     ]
    }
   ],
   "source": [
    "def load_qa_database(filepath: str = \"Q&A_db_practice.json\") -> list[dict]:\n",
    "    \"\"\"Load the question-answer database from JSON.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "# Test loading\n",
    "qa_db = load_qa_database()\n",
    "print(f\"‚úÖ Loaded {len(qa_db)} questions\")\n",
    "print(f\"Sample question: {qa_db[0]['question']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 2. QUESTION SELECTION\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Question ID: f15a3c56-e3e6-4c5b-861e-f7cf3a2d25c3\n",
      "‚úÖ Question: Recurrent Neural Network (RNN)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_question(strategy: str = \"random\", qa_db: Optional[list] = None) -> dict:\n",
    "    \"\"\"\n",
    "    Select a question from the repository.\n",
    "    \n",
    "    Args:\n",
    "        strategy: Selection method (\"random\" or \"sequential\")\n",
    "        qa_db: Pre-loaded Q&A database (loads if None)\n",
    "    \n",
    "    Returns:\n",
    "        {\n",
    "            \"question_id\": \"uuid\",\n",
    "            \"question\": \"text\",\n",
    "            \"target_answer\": \"text\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    if qa_db is None:\n",
    "        qa_db = load_qa_database()\n",
    "    \n",
    "    if strategy == \"random\":\n",
    "        selected = random.choice(qa_db)\n",
    "    elif strategy == \"sequential\":\n",
    "        selected = qa_db[0]  # Simple version - extend with state tracking\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "    \n",
    "    return {\n",
    "        \"question_id\": str(uuid.uuid4()),\n",
    "        \"question\": selected[\"question\"],\n",
    "        \"target_answer\": selected[\"answer\"]\n",
    "    }\n",
    "\n",
    "# Test\n",
    "test_q = get_question(qa_db=qa_db)\n",
    "print(f\"‚úÖ Question ID: {test_q['question_id']}\")\n",
    "print(f\"‚úÖ Question: {test_q['question']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. ROUGE METRICS\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ROUGE scores: {'r1': 0.636, 'r2': 0.3, 'rl': 0.455}\n"
     ]
    }
   ],
   "source": [
    "def compute_rouge(target: str, answer: str) -> dict:\n",
    "    \"\"\"\n",
    "    Compute ROUGE-1, ROUGE-2, ROUGE-L F1 scores.\n",
    "    \n",
    "    Args:\n",
    "        target: Reference answer\n",
    "        answer: User's answer\n",
    "    \n",
    "    Returns:\n",
    "        {\"r1\": float, \"r2\": float, \"rl\": float}\n",
    "    \"\"\"\n",
    "    if not ROUGE_AVAILABLE:\n",
    "        return {\"r1\": 0.0, \"r2\": 0.0, \"rl\": 0.0}\n",
    "    \n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], \n",
    "                                       use_stemmer=True)\n",
    "    scores = scorer.score(target, answer)\n",
    "    \n",
    "    return {\n",
    "        \"r1\": round(scores['rouge1'].fmeasure, 3),\n",
    "        \"r2\": round(scores['rouge2'].fmeasure, 3),\n",
    "        \"rl\": round(scores['rougeL'].fmeasure, 3)\n",
    "    }\n",
    "\n",
    "# Test\n",
    "test_target = \"Machine learning is a method of data analysis that automates analytical model building.\"\n",
    "test_answer = \"Machine learning automates model building using data analysis techniques.\"\n",
    "rouge_result = compute_rouge(test_target, test_answer)\n",
    "print(f\"‚úÖ ROUGE scores: {rouge_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 4. LLM-BASED EVALUATION\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prompt template defined\n"
     ]
    }
   ],
   "source": [
    "EVALUATION_PROMPT = \"\"\"You are an expert AI/ML educator evaluating student answers.\n",
    "\n",
    "**Question:** {question}\n",
    "\n",
    "**Target Answer:** {target}\n",
    "\n",
    "**Student Answer:** {answer}\n",
    "\n",
    "Evaluate the student's answer on three dimensions:\n",
    "1. **Correctness**: Are the core concepts accurate?\n",
    "2. **Completeness**: Does it cover key aspects of the target?\n",
    "3. **Precision**: Is the terminology and explanation clear?\n",
    "\n",
    "Respond ONLY with valid JSON (no markdown, no extra text):\n",
    "\n",
    "{{\n",
    "  \"score_0_100\": <integer 0-100>,\n",
    "  \"correctness\": \"<1-2 sentence assessment>\",\n",
    "  \"completeness\": \"<1-2 sentence assessment>\",\n",
    "  \"precision\": \"<1-2 sentence assessment>\",\n",
    "  \"rationale\": [\"<point 1>\", \"<point 2>\", \"<point 3>\"]\n",
    "}}\n",
    "\n",
    "Scoring guide:\n",
    "- 90-100: Excellent (accurate, comprehensive, precise)\n",
    "- 70-89: Good (mostly correct, minor gaps)\n",
    "- 50-69: Partial (some understanding, significant gaps)\n",
    "- 0-49: Poor (fundamental errors or missing concepts)\n",
    "\n",
    "Remember: Return ONLY the JSON object, nothing else.\"\"\"\n",
    "\n",
    "print(\"‚úÖ Prompt template defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 5. MAIN EVALUATION FUNCTION\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå LLM evaluation error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "‚úÖ LLM evaluation: 50/100\n"
     ]
    }
   ],
   "source": [
    "def evaluate_with_llm(question: str, target: str, answer: str) -> dict:\n",
    "    \"\"\"\n",
    "    Use LLM to evaluate answer quality.\n",
    "    \n",
    "    Args:\n",
    "        question: The question text\n",
    "        target: Target answer\n",
    "        answer: User's answer\n",
    "    \n",
    "    Returns:\n",
    "        {\n",
    "            \"score_0_100\": int,\n",
    "            \"correctness\": str,\n",
    "            \"completeness\": str,\n",
    "            \"precision\": str,\n",
    "            \"rationale\": list[str]\n",
    "        }\n",
    "    \"\"\"\n",
    "    if not LLM_AVAILABLE:\n",
    "        return {\n",
    "            \"score_0_100\": 50,\n",
    "            \"correctness\": \"LLM not available\",\n",
    "            \"completeness\": \"Cannot assess without LLM\",\n",
    "            \"precision\": \"Fallback mode\",\n",
    "            \"rationale\": [\"LLM client not configured\", \"Set OPENAI_API_KEY environment variable\"]\n",
    "        }\n",
    "    \n",
    "    # Get API key from environment\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        return {\n",
    "            \"score_0_100\": 50,\n",
    "            \"correctness\": \"API key missing\",\n",
    "            \"completeness\": \"Set OPENAI_API_KEY\",\n",
    "            \"precision\": \"Cannot evaluate\",\n",
    "            \"rationale\": [\"Set environment variable: OPENAI_API_KEY\"]\n",
    "        }\n",
    "    \n",
    "    client = OpenAI(api_key=api_key)\n",
    "    \n",
    "    prompt = EVALUATION_PROMPT.format(\n",
    "        question=question,\n",
    "        target=target,\n",
    "        answer=answer\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a precise evaluator. Return only valid JSON.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        result_text = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Clean potential markdown\n",
    "        if result_text.startswith(\"```\"):\n",
    "            result_text = result_text.split(\"```\")[1]\n",
    "            if result_text.startswith(\"json\"):\n",
    "                result_text = result_text[4:]\n",
    "            result_text = result_text.rsplit(\"```\", 1)[0]\n",
    "        \n",
    "        evaluation = json.loads(result_text)\n",
    "        \n",
    "        # Validate required keys\n",
    "        required = {\"score_0_100\", \"correctness\", \"completeness\", \"precision\", \"rationale\"}\n",
    "        if not required.issubset(evaluation.keys()):\n",
    "            raise ValueError(\"Missing required keys in LLM response\")\n",
    "        \n",
    "        return evaluation\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå LLM evaluation error: {e}\")\n",
    "        return {\n",
    "            \"score_0_100\": 50,\n",
    "            \"correctness\": \"Evaluation failed\",\n",
    "            \"completeness\": \"System error\",\n",
    "            \"precision\": \"Could not process\",\n",
    "            \"rationale\": [f\"Error: {str(e)}\"]\n",
    "        }\n",
    "\n",
    "# Test (will use fallback if no API key)\n",
    "test_eval = evaluate_with_llm(\n",
    "    question=\"What is overfitting?\",\n",
    "    target=\"Overfitting occurs when a model learns the training data too well, including noise, reducing generalization.\",\n",
    "    answer=\"It's when the model memorizes training data.\"\n",
    ")\n",
    "print(f\"‚úÖ LLM evaluation: {test_eval['score_0_100']}/100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 6.  Main Evaluation Function\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå LLM evaluation error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "‚úÖ Evaluation complete: 36/100\n"
     ]
    }
   ],
   "source": [
    "def evaluate_answer(\n",
    "    question: str, \n",
    "    target: str, \n",
    "    answer: str, \n",
    "    *, \n",
    "    rouge: bool = True,\n",
    "    question_id: Optional[str] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Comprehensive answer evaluation combining LLM judgment and ROUGE metrics.\n",
    "    \n",
    "    Args:\n",
    "        question: Question text\n",
    "        target: Reference answer\n",
    "        answer: User's submitted answer\n",
    "        rouge: Whether to compute ROUGE scores\n",
    "        question_id: Optional question identifier\n",
    "    \n",
    "    Returns:\n",
    "        {\n",
    "            \"eval_id\": \"uuid\",\n",
    "            \"question_id\": \"string\",\n",
    "            \"model_judgment\": {...},\n",
    "            \"rouge\": {\"r1\": float, \"r2\": float, \"rl\": float},\n",
    "            \"final_score_0_100\": int,\n",
    "            \"timestamp\": \"iso-datetime\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    eval_id = str(uuid.uuid4())\n",
    "    \n",
    "    # 1. LLM evaluation\n",
    "    llm_judgment = evaluate_with_llm(question, target, answer)\n",
    "    \n",
    "    # 2. ROUGE metrics\n",
    "    rouge_scores = compute_rouge(target, answer) if rouge else {\"r1\": 0.0, \"r2\": 0.0, \"rl\": 0.0}\n",
    "    \n",
    "    # 3. Combined score (70% LLM, 30% ROUGE)\n",
    "    rouge_avg = (rouge_scores[\"r1\"] + rouge_scores[\"r2\"] + rouge_scores[\"rl\"]) / 3\n",
    "    final_score = int(0.7 * llm_judgment[\"score_0_100\"] + 0.3 * rouge_avg * 100)\n",
    "    \n",
    "    return {\n",
    "        \"eval_id\": eval_id,\n",
    "        \"question_id\": question_id or \"unknown\",\n",
    "        \"model_judgment\": llm_judgment,\n",
    "        \"rouge\": rouge_scores,\n",
    "        \"final_score_0_100\": final_score,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "# Test\n",
    "test_result = evaluate_answer(\n",
    "    question=test_q[\"question\"],\n",
    "    target=test_q[\"target_answer\"],\n",
    "    answer=\"It is a technique used in machine learning.\",\n",
    "    question_id=test_q[\"question_id\"]\n",
    ")\n",
    "print(f\"‚úÖ Evaluation complete: {test_result['final_score_0_100']}/100\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 8: Sentiment Analysis via LLM\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sentiment analysis error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "‚úÖ Sentiment: neutral (confidence: 0.0)\n"
     ]
    }
   ],
   "source": [
    "SENTIMENT_PROMPT = \"\"\"Analyze the sentiment of this user feedback comment.\n",
    "\n",
    "**Comment:** {comment}\n",
    "\n",
    "Classify the sentiment as one of: positive, negative, or neutral.\n",
    "\n",
    "Respond ONLY with valid JSON (no markdown):\n",
    "\n",
    "{{\n",
    "  \"sentiment\": \"<positive|negative|neutral>\",\n",
    "  \"confidence\": <float 0.0-1.0>,\n",
    "  \"reasoning\": \"<1 sentence explanation>\"\n",
    "}}\"\"\"\n",
    "\n",
    "def analyze_sentiment_llm(comment: Optional[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze sentiment of user comment using LLM.\n",
    "    \n",
    "    Args:\n",
    "        comment: User's feedback text\n",
    "    \n",
    "    Returns:\n",
    "        {\n",
    "            \"sentiment\": \"positive|negative|neutral\",\n",
    "            \"confidence\": float,\n",
    "            \"reasoning\": str\n",
    "        }\n",
    "    \"\"\"\n",
    "    if not comment:\n",
    "        return {\n",
    "            \"sentiment\": \"neutral\",\n",
    "            \"confidence\": 1.0,\n",
    "            \"reasoning\": \"No comment provided\"\n",
    "        }\n",
    "    \n",
    "    if not LLM_AVAILABLE:\n",
    "        return {\n",
    "            \"sentiment\": \"neutral\",\n",
    "            \"confidence\": 0.0,\n",
    "            \"reasoning\": \"LLM not available for sentiment analysis\"\n",
    "        }\n",
    "    \n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        return {\n",
    "            \"sentiment\": \"neutral\",\n",
    "            \"confidence\": 0.0,\n",
    "            \"reasoning\": \"API key not configured\"\n",
    "        }\n",
    "    \n",
    "    client = OpenAI(api_key=api_key)\n",
    "    \n",
    "    prompt = SENTIMENT_PROMPT.format(comment=comment)\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a sentiment analysis expert. Return only JSON.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        \n",
    "        result_text = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Clean markdown\n",
    "        if result_text.startswith(\"```\"):\n",
    "            result_text = result_text.split(\"```\")[1]\n",
    "            if result_text.startswith(\"json\"):\n",
    "                result_text = result_text[4:]\n",
    "            result_text = result_text.rsplit(\"```\", 1)[0]\n",
    "        \n",
    "        sentiment = json.loads(result_text)\n",
    "        \n",
    "        # Validate\n",
    "        if \"sentiment\" not in sentiment:\n",
    "            raise ValueError(\"Missing sentiment field\")\n",
    "        \n",
    "        return sentiment\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Sentiment analysis error: {e}\")\n",
    "        return {\n",
    "            \"sentiment\": \"neutral\",\n",
    "            \"confidence\": 0.0,\n",
    "            \"reasoning\": f\"Error: {str(e)}\"\n",
    "        }\n",
    "\n",
    "# Test\n",
    "test_sentiment = analyze_sentiment_llm(\"This evaluation was very helpful and clear!\")\n",
    "print(f\"‚úÖ Sentiment: {test_sentiment['sentiment']} (confidence: {test_sentiment.get('confidence', 0)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 9. Feedback Recording Function\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sentiment analysis error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "‚úÖ Feedback recorded: neutral\n",
      "   Reasoning: Error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    }
   ],
   "source": [
    "FEEDBACK_DB = []  # In-memory storage\n",
    "\n",
    "def record_feedback(\n",
    "    eval_id: str, \n",
    "    labels: list[str], \n",
    "    comment: Optional[str] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Record user feedback on an evaluation.\n",
    "    \n",
    "    Args:\n",
    "        eval_id: Evaluation identifier\n",
    "        labels: Selected feedback labels\n",
    "        comment: Optional free-text feedback\n",
    "    \n",
    "    Returns:\n",
    "        {\n",
    "            \"feedback_id\": \"uuid\",\n",
    "            \"eval_id\": str,\n",
    "            \"labels\": list[str],\n",
    "            \"comment\": str,\n",
    "            \"sentiment_analysis\": dict,\n",
    "            \"timestamp\": str\n",
    "        }\n",
    "    \"\"\"\n",
    "    feedback_id = str(uuid.uuid4())\n",
    "    \n",
    "    # Use LLM for sentiment analysis\n",
    "    sentiment = analyze_sentiment_llm(comment)\n",
    "    \n",
    "    feedback_entry = {\n",
    "        \"feedback_id\": feedback_id,\n",
    "        \"eval_id\": eval_id,\n",
    "        \"labels\": labels,\n",
    "        \"comment\": comment,\n",
    "        \"sentiment_analysis\": sentiment,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    FEEDBACK_DB.append(feedback_entry)\n",
    "    \n",
    "    return feedback_entry\n",
    "\n",
    "# Test\n",
    "test_feedback = record_feedback(\n",
    "    eval_id=test_result[\"eval_id\"],\n",
    "    labels=[\"useful\", \"clear\"],\n",
    "    comment=\"Very helpful explanation of my mistakes!\"\n",
    ")\n",
    "print(f\"‚úÖ Feedback recorded: {test_feedback['sentiment_analysis']['sentiment']}\")\n",
    "print(f\"   Reasoning: {test_feedback['sentiment_analysis']['reasoning']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 10. Debug Helper: Generate Novice Answer\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated novice answer: A recurrent neural network (RNN) is a type of artificial neural network that processes sequential da... I think.\n"
     ]
    }
   ],
   "source": [
    "def generate_novice_answer(question: str, target: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a simplified/incomplete answer for testing.\n",
    "    \n",
    "    Args:\n",
    "        question: The question\n",
    "        target: Target answer\n",
    "    \n",
    "    Returns:\n",
    "        Simulated novice answer\n",
    "    \"\"\"\n",
    "    # Extract first sentence or first 100 chars\n",
    "    first_part = target.split('.')[0] if '.' in target else target[:100]\n",
    "    \n",
    "    templates = [\n",
    "        f\"{first_part}... I think.\",\n",
    "        f\"I believe {first_part.lower()}\",\n",
    "        f\"It's related to {' '.join(first_part.split()[-5:])}\",\n",
    "        \"I'm not completely sure, but it has something to do with the concept mentioned.\"\n",
    "    ]\n",
    "    \n",
    "    return random.choice(templates)\n",
    "\n",
    "# Test\n",
    "novice = generate_novice_answer(test_q[\"question\"], test_q[\"target_answer\"])\n",
    "print(f\"‚úÖ Generated novice answer: {novice}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 11. Full Pipeline Test\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FULL PIPELINE TEST\n",
      "============================================================\n",
      "\n",
      "üìù Question: Data Science\n",
      "\n",
      "üí≠ Simulated Answer: I believe data science is an interdisciplinary practice that extracts knowledge and actionable insights from s\n",
      "\n",
      "‚è≥ Evaluating...\n",
      "‚ùå LLM evaluation error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "\n",
      "üìä Results:\n",
      "  Final Score: 50/100\n",
      "  LLM Score: 50/100\n",
      "  ROUGE-1: 0.52\n",
      "  ROUGE-2: 0.5\n",
      "  ROUGE-L: 0.52\n",
      "\n",
      "üí° Rationale:\n",
      "  ‚Ä¢ Error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "‚ùå Sentiment analysis error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "\n",
      "‚úÖ Feedback:\n",
      "  Sentiment: neutral\n",
      "  Reasoning: Error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FULL PIPELINE TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Get question\n",
    "q = get_question(strategy=\"random\", qa_db=qa_db)\n",
    "print(f\"\\nüìù Question: {q['question']}\")\n",
    "\n",
    "# 2. Simulate answer (debug mode)\n",
    "user_answer = generate_novice_answer(q['question'], q['target_answer'])\n",
    "print(f\"\\nüí≠ Simulated Answer: {user_answer}\")\n",
    "\n",
    "# 3. Evaluate\n",
    "print(\"\\n‚è≥ Evaluating...\")\n",
    "result = evaluate_answer(\n",
    "    question=q['question'],\n",
    "    target=q['target_answer'],\n",
    "    answer=user_answer,\n",
    "    question_id=q['question_id']\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"  Final Score: {result['final_score_0_100']}/100\")\n",
    "print(f\"  LLM Score: {result['model_judgment']['score_0_100']}/100\")\n",
    "print(f\"  ROUGE-1: {result['rouge']['r1']}\")\n",
    "print(f\"  ROUGE-2: {result['rouge']['r2']}\")\n",
    "print(f\"  ROUGE-L: {result['rouge']['rl']}\")\n",
    "print(f\"\\nüí° Rationale:\")\n",
    "for point in result['model_judgment']['rationale']:\n",
    "    print(f\"  ‚Ä¢ {point}\")\n",
    "\n",
    "# 4. Collect feedback\n",
    "feedback = record_feedback(\n",
    "    eval_id=result['eval_id'],\n",
    "    labels=[\"useful\", \"rigorous\"],\n",
    "    comment=\"The evaluation helped me understand where I went wrong.\"\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Feedback:\")\n",
    "print(f\"  Sentiment: {feedback['sentiment_analysis']['sentiment']}\")\n",
    "print(f\"  Reasoning: {feedback['sentiment_analysis']['reasoning']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 12. Export to model_app.py\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_app.py\n",
    "\"\"\"\n",
    "Q&A Evaluator Core Logic - Production Module\n",
    "Assignment 11.02 - LLM Applications\n",
    "\n",
    "This module provides core functions for:\n",
    "- Question selection from repository\n",
    "- Answer evaluation using LLM + ROUGE metrics\n",
    "- Feedback collection with sentiment analysis\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import uuid\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "# ROUGE metrics\n",
    "try:\n",
    "    from rouge_score import rouge_scorer\n",
    "    ROUGE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    ROUGE_AVAILABLE = False\n",
    "\n",
    "# LLM client\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    LLM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LLM_AVAILABLE = False\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# DATA LOADING\n",
    "# ============================================================\n",
    "\n",
    "def load_qa_database(filepath: str = \"Q&A_db_practice.json\") -> list[dict]:\n",
    "    \"\"\"Load the question-answer database from JSON.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# QUESTION SELECTION\n",
    "# ============================================================\n",
    "\n",
    "def get_question(strategy: str = \"random\", qa_db: Optional[list] = None) -> dict:\n",
    "    \"\"\"\n",
    "    Select a question from the repository.\n",
    "    \n",
    "    Args:\n",
    "        strategy: Selection method (\"random\" or \"sequential\")\n",
    "        qa_db: Pre-loaded Q&A database\n",
    "    \n",
    "    Returns:\n",
    "        dict with question_id, question, target_answer\n",
    "    \"\"\"\n",
    "    if qa_db is None:\n",
    "        qa_db = load_qa_database()\n",
    "    \n",
    "    if strategy == \"random\":\n",
    "        selected = random.choice(qa_db)\n",
    "    elif strategy == \"sequential\":\n",
    "        selected = qa_db[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "    \n",
    "    return {\n",
    "        \"question_id\": str(uuid.uuid4()),\n",
    "        \"question\": selected[\"question\"],\n",
    "        \"target_answer\": selected[\"answer\"]\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ROUGE METRICS\n",
    "# ============================================================\n",
    "\n",
    "def compute_rouge(target: str, answer: str) -> dict:\n",
    "    \"\"\"\n",
    "    Compute ROUGE-1, ROUGE-2, ROUGE-L F1 scores.\n",
    "    \n",
    "    Returns:\n",
    "        dict with r1, r2, rl keys\n",
    "    \"\"\"\n",
    "    if not ROUGE_AVAILABLE:\n",
    "        return {\"r1\": 0.0, \"r2\": 0.0, \"rl\": 0.0}\n",
    "    \n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], \n",
    "                                       use_stemmer=True)\n",
    "    scores = scorer.score(target, answer)\n",
    "    \n",
    "    return {\n",
    "        \"r1\": round(scores['rouge1'].fmeasure, 3),\n",
    "        \"r2\": round(scores['rouge2'].fmeasure, 3),\n",
    "        \"rl\": round(scores['rougeL'].fmeasure, 3)\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LLM EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "EVALUATION_PROMPT = \"\"\"You are an expert AI/ML educator evaluating student answers.\n",
    "\n",
    "**Question:** {question}\n",
    "\n",
    "**Target Answer:** {target}\n",
    "\n",
    "**Student Answer:** {answer}\n",
    "\n",
    "Evaluate the student's answer on three dimensions:\n",
    "1. **Correctness**: Are the core concepts accurate?\n",
    "2. **Completeness**: Does it cover key aspects of the target?\n",
    "3. **Precision**: Is the terminology and explanation clear?\n",
    "\n",
    "Respond ONLY with valid JSON (no markdown, no extra text):\n",
    "\n",
    "{{\n",
    "  \"score_0_100\": <integer 0-100>,\n",
    "  \"correctness\": \"<1-2 sentence assessment>\",\n",
    "  \"completeness\": \"<1-2 sentence assessment>\",\n",
    "  \"precision\": \"<1-2 sentence assessment>\",\n",
    "  \"rationale\": [\"<point 1>\", \"<point 2>\", \"<point 3>\"]\n",
    "}}\n",
    "\n",
    "Scoring guide:\n",
    "- 90-100: Excellent (accurate, comprehensive, precise)\n",
    "- 70-89: Good (mostly correct, minor gaps)\n",
    "- 50-69: Partial (some understanding, significant gaps)\n",
    "- 0-49: Poor (fundamental errors or missing concepts)\n",
    "\n",
    "Remember: Return ONLY the JSON object, nothing else.\"\"\"\n",
    "\n",
    "\n",
    "def evaluate_with_llm(question: str, target: str, answer: str) -> dict:\n",
    "    \"\"\"Use LLM to evaluate answer quality.\"\"\"\n",
    "    if not LLM_AVAILABLE:\n",
    "        return {\n",
    "            \"score_0_100\": 50,\n",
    "            \"correctness\": \"LLM not available\",\n",
    "            \"completeness\": \"Cannot assess\",\n",
    "            \"precision\": \"Fallback mode\",\n",
    "            \"rationale\": [\"LLM client not configured\"]\n",
    "        }\n",
    "    \n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        return {\n",
    "            \"score_0_100\": 50,\n",
    "            \"correctness\": \"API key missing\",\n",
    "            \"completeness\": \"Set OPENAI_API_KEY\",\n",
    "            \"precision\": \"Cannot evaluate\",\n",
    "            \"rationale\": [\"Environment variable OPENAI_API_KEY required\"]\n",
    "        }\n",
    "    \n",
    "    client = OpenAI(api_key=api_key)\n",
    "    prompt = EVALUATION_PROMPT.format(question=question, target=target, answer=answer)\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a precise evaluator. Return only valid JSON.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        result_text = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Clean markdown if present\n",
    "        if result_text.startswith(\"```\"):\n",
    "            result_text = result_text.split(\"```\")[1]\n",
    "            if result_text.startswith(\"json\"):\n",
    "                result_text = result_text[4:]\n",
    "            result_text = result_text.rsplit(\"```\", 1)[0]\n",
    "        \n",
    "        evaluation = json.loads(result_text)\n",
    "        \n",
    "        required = {\"score_0_100\", \"correctness\", \"completeness\", \"precision\", \"rationale\"}\n",
    "        if not required.issubset(evaluation.keys()):\n",
    "            raise ValueError(\"Missing required keys\")\n",
    "        \n",
    "        return evaluation\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"score_0_100\": 50,\n",
    "            \"correctness\": \"Evaluation failed\",\n",
    "            \"completeness\": \"System error\",\n",
    "            \"precision\": \"Could not process\",\n",
    "            \"rationale\": [f\"Error: {str(e)}\"]\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_answer(\n",
    "    question: str, \n",
    "    target: str, \n",
    "    answer: str, \n",
    "    *, \n",
    "    rouge: bool = True,\n",
    "    question_id: Optional[str] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Comprehensive answer evaluation.\n",
    "    \n",
    "    Returns:\n",
    "        dict with eval_id, question_id, model_judgment, rouge, final_score_0_100, timestamp\n",
    "    \"\"\"\n",
    "    eval_id = str(uuid.uuid4())\n",
    "    \n",
    "    llm_judgment = evaluate_with_llm(question, target, answer)\n",
    "    rouge_scores = compute_rouge(target, answer) if rouge else {\"r1\": 0.0, \"r2\": 0.0, \"rl\": 0.0}\n",
    "    \n",
    "    rouge_avg = (rouge_scores[\"r1\"] + rouge_scores[\"r2\"] + rouge_scores[\"rl\"]) / 3\n",
    "    final_score = int(0.7 * llm_judgment[\"score_0_100\"] + 0.3 * rouge_avg * 100)\n",
    "    \n",
    "    return {\n",
    "        \"eval_id\": eval_id,\n",
    "        \"question_id\": question_id or \"unknown\",\n",
    "        \"model_judgment\": llm_judgment,\n",
    "        \"rouge\": rouge_scores,\n",
    "        \"final_score_0_100\": final_score,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SENTIMENT ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "SENTIMENT_PROMPT = \"\"\"Analyze the sentiment of this user feedback comment.\n",
    "\n",
    "**Comment:** {comment}\n",
    "\n",
    "Classify the sentiment as one of: positive, negative, or neutral.\n",
    "\n",
    "Respond ONLY with valid JSON (no markdown):\n",
    "\n",
    "{{\n",
    "  \"sentiment\": \"<positive|negative|neutral>\",\n",
    "  \"confidence\": <float 0.0-1.0>,\n",
    "  \"reasoning\": \"<1 sentence explanation>\"\n",
    "}}\"\"\"\n",
    "\n",
    "\n",
    "def analyze_sentiment_llm(comment: Optional[str]) -> dict:\n",
    "    \"\"\"Analyze sentiment using LLM.\"\"\"\n",
    "    if not comment:\n",
    "        return {\n",
    "            \"sentiment\": \"neutral\",\n",
    "            \"confidence\": 1.0,\n",
    "            \"reasoning\": \"No comment provided\"\n",
    "        }\n",
    "    \n",
    "    if not LLM_AVAILABLE:\n",
    "        return {\n",
    "            \"sentiment\": \"neutral\",\n",
    "            \"confidence\": 0.0,\n",
    "            \"reasoning\": \"LLM not available\"\n",
    "        }\n",
    "    \n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        return {\n",
    "            \"sentiment\": \"neutral\",\n",
    "            \"confidence\": 0.0,\n",
    "            \"reasoning\": \"API key not configured\"\n",
    "        }\n",
    "    \n",
    "    client = OpenAI(api_key=api_key)\n",
    "    prompt = SENTIMENT_PROMPT.format(comment=comment)\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a sentiment analysis expert. Return only JSON.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        \n",
    "        result_text = response.choices[0].message.content.strip()\n",
    "        \n",
    "        if result_text.startswith(\"```\"):\n",
    "            result_text = result_text.split(\"```\")[1]\n",
    "            if result_text.startswith(\"json\"):\n",
    "                result_text = result_text[4:]\n",
    "            result_text = result_text.rsplit(\"```\", 1)[0]\n",
    "        \n",
    "        sentiment = json.loads(result_text)\n",
    "        \n",
    "        if \"sentiment\" not in sentiment:\n",
    "            raise ValueError(\"Missing sentiment field\")\n",
    "        \n",
    "        return sentiment\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"sentiment\": \"neutral\",\n",
    "            \"confidence\": 0.0,\n",
    "            \"reasoning\": f\"Error: {str(e)}\"\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FEEDBACK RECORDING\n",
    "# ============================================================\n",
    "\n",
    "FEEDBACK_DB = []\n",
    "\n",
    "\n",
    "def record_feedback(\n",
    "    eval_id: str, \n",
    "    labels: list[str], \n",
    "    comment: Optional[str] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Record user feedback with LLM sentiment analysis.\n",
    "    \n",
    "    Returns:\n",
    "        dict with feedback_id, eval_id, labels, comment, sentiment_analysis, timestamp\n",
    "    \"\"\"\n",
    "    feedback_id = str(uuid.uuid4())\n",
    "    sentiment = analyze_sentiment_llm(comment)\n",
    "    \n",
    "    feedback_entry = {\n",
    "        \"feedback_id\": feedback_id,\n",
    "        \"eval_id\": eval_id,\n",
    "        \"labels\": labels,\n",
    "        \"comment\": comment,\n",
    "        \"sentiment_analysis\": sentiment,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    FEEDBACK_DB.append(feedback_entry)\n",
    "    return feedback_entry\n",
    "\n",
    "# ============================================================\n",
    "# DEBUG UTILITIES\n",
    "# ============================================================\n",
    "\n",
    "def generate_novice_answer(question: str, target: str) -> str:\n",
    "    \"\"\"Generate a simplified answer for testing.\"\"\"\n",
    "    first_part = target.split('.')[0] if '.' in target else target[:100]\n",
    "    templates = [\n",
    "        f\"{first_part}... I think.\",\n",
    "        f\"I believe {first_part.lower()}\",\n",
    "        f\"It's related to {' '.join(first_part.split()[-5:])}\",\n",
    "        \"I'm not sure, but it relates to the concept.\"\n",
    "    ]\n",
    "    return random.choice(templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ model_app.py created successfully!\n",
      "‚úÖ File contains 360 lines\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.path.exists(\"model_app.py\"):\n",
    "    print(\"‚úÖ model_app.py created successfully!\")\n",
    "    with open(\"model_app.py\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        print(f\"‚úÖ File contains {len(lines)} lines\")\n",
    "else:\n",
    "    print(\"‚ùå model_app.py was NOT created\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
