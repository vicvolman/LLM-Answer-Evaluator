{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42751043",
   "metadata": {},
   "source": [
    "# Assignment 11 – Part 1  \n",
    "## NER preprocessing and professor profile cleaning\n",
    "\n",
    "This notebook:\n",
    "\n",
    "1. Loads the raw `teachers_db_practice.csv` file.\n",
    "2. Cleans and splits each professor profile into **Corporate Experience**, **Academic Experience**, and **Academic Background** sections.\n",
    "3. Runs a Named Entity Recognition (NER) model to detect organizations and locations.\n",
    "4. Normalizes and clusters entity strings (e.g. “ie university” → “IE”).\n",
    "5. Builds a per-professor dictionary with:\n",
    "   - Corporate organizations and locations  \n",
    "   - Academic organizations  \n",
    "   - Degrees / education  \n",
    "   - Academic subjects\n",
    "6. Saves a compact JSON file (`cleaned_professors.json`) used later to build the knowledge graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fc964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# NER + utilities\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def ensure_package(pkg_name: str):\n",
    "    \"\"\"\n",
    "    Import a package if available; otherwise install it with pip and then import.\n",
    "    This allows running the notebook on a clean machine with just 'Run All'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        __import__(pkg_name)\n",
    "    except ImportError:\n",
    "        print(f\"Installing missing package: {pkg_name}\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg_name])\n",
    "\n",
    "for pkg in [\"transformers\", \"torch\", \"tqdm\", \"rapidfuzz\", \"pyarrow\"]:\n",
    "    ensure_package(pkg)\n",
    "\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d96ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path structure:\n",
    "#   data/raw/teachers_db_practice.csv\n",
    "#   data/processed/  (output folder used later)\n",
    "\n",
    "raw_path = \"../data/raw/teachers_db_practice.csv\"\n",
    "df = pd.read_csv(raw_path)\n",
    "\n",
    "print(\"Loaded dataset:\")\n",
    "print(\"  shape:\", df.shape)\n",
    "print(\"  columns:\", list(df.columns))\n",
    "\n",
    "# Inspect a single raw profile to understand the HTML-ish structure\n",
    "display(df.loc[0, [\"full_info\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117d803b",
   "metadata": {},
   "source": [
    "### Clean and normalize raw profile text\n",
    "\n",
    "The `full_info` field is stored as an HTML-like blob with tags and entities (e.g., `<h4>`, `&amp;`).  \n",
    "Before running NER, I normalize each profile by:\n",
    "\n",
    "- Removing HTML tags and collapsing whitespace.\n",
    "- Converting `&amp;` variants back to `&`.\n",
    "\n",
    "This gives a stable, plain-text representation (`clean_text`) that is easier for the NER model and for later regex-based parsing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda42c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the HTML-ish blob into a single text field per professor\n",
    "\n",
    "def clean_html_text(html: str) -> str:\n",
    "    html = html or \"\"\n",
    "    # collapse &amp; variants\n",
    "    html = re.sub(r\"&\\s*amp;?\", \"&\", html, flags=re.I)\n",
    "    # remove all tags\n",
    "    html = re.sub(r\"<.*?>\", \" \", html)\n",
    "    # collapse whitespace\n",
    "    html = re.sub(r\"\\s+\", \" \", html).strip()\n",
    "    return html\n",
    "\n",
    "df[\"clean_text\"] = df[\"full_info\"].fillna(\"\").apply(clean_html_text)\n",
    "\n",
    "df[\"clean_text\"].head(3).to_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adddc783",
   "metadata": {},
   "source": [
    "### Split profiles into logical sections\n",
    "\n",
    "Many CVs have explicit section headers: **CORPORATE EXPERIENCE**, **ACADEMIC EXPERIENCE**, and  \n",
    "**ACADEMIC BACKGROUND**. I use regexes to:\n",
    "\n",
    "- Detect these headers in the original HTML string.\n",
    "- Slice the profile into three section-specific text fields.\n",
    "\n",
    "If the headers are missing, the entire block is treated as corporate experience.  \n",
    "Working per-section makes it easier to decide whether a given organization or location belongs to  \n",
    "corporate vs. academic parts of the profile.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1137ddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each profile into (possibly empty) sections:\n",
    "#   - CORPORATE EXPERIENCE\n",
    "#   - ACADEMIC EXPERIENCE\n",
    "#   - ACADEMIC BACKGROUND\n",
    "\n",
    "SECTION_HEADERS = {\n",
    "    \"corp\": r\"(?:<h4>\\s*CORPORATE EXPERIENCE\\s*</h4>|CORPORATE EXPERIENCE)\",\n",
    "    \"acadexp\": r\"(?:<h4>\\s*ACADEMIC EXPERIENCE\\s*</h4>|ACADEMIC EXPERIENCE)\",\n",
    "    \"acadbg\": r\"(?:<h4>\\s*ACADEMIC BACKGROUND\\s*</h4>|ACADEMIC BACKGROUND)\",\n",
    "}\n",
    "\n",
    "def strip_html(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = re.sub(r\"&\\s*amp;?\", \"&\", s, flags=re.I)\n",
    "    s = re.sub(r\"<.*?>\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def extract_sections(raw: str) -> pd.Series:\n",
    "    text = raw if isinstance(raw, str) else \"\"\n",
    "\n",
    "    def find_idx(pattern: str):\n",
    "        m = re.search(pattern, text, flags=re.I)\n",
    "        return m.start() if m else None\n",
    "\n",
    "    i_corp    = find_idx(SECTION_HEADERS[\"corp\"])\n",
    "    i_acadexp = find_idx(SECTION_HEADERS[\"acadexp\"])\n",
    "    i_acadbg  = find_idx(SECTION_HEADERS[\"acadbg\"])\n",
    "\n",
    "    def slice_between(start, end):\n",
    "        if start is None:\n",
    "            return \"\"\n",
    "        end = len(text) if end is None else end\n",
    "        return strip_html(text[start:end])\n",
    "\n",
    "    idxs = sorted(\n",
    "        [(k, v) for k, v in [(\"corp\", i_corp), (\"acadexp\", i_acadexp), (\"acadbg\", i_acadbg)] if v is not None],\n",
    "        key=lambda x: x[1],\n",
    "    )\n",
    "\n",
    "    corp_txt = acadexp_txt = acadbg_txt = \"\"\n",
    "    if idxs:\n",
    "        for j, (label, start) in enumerate(idxs):\n",
    "            end = idxs[j + 1][1] if j + 1 < len(idxs) else None\n",
    "            chunk = slice_between(start, end)\n",
    "            if label == \"corp\":\n",
    "                corp_txt = chunk\n",
    "            elif label == \"acadexp\":\n",
    "                acadexp_txt = chunk\n",
    "            elif label == \"acadbg\":\n",
    "                acadbg_txt = chunk\n",
    "    else:\n",
    "        # if no headers, treat entire blob as \"corporate\" so we don't lose information\n",
    "        corp_txt = strip_html(text)\n",
    "\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"corp_text\": corp_txt,\n",
    "            \"acadexp_text\": acadexp_txt,\n",
    "            \"acadbg_text\": acadbg_txt,\n",
    "        }\n",
    "    )\n",
    "\n",
    "sec_df = df[\"full_info\"].apply(extract_sections)\n",
    "df = pd.concat([df, sec_df], axis=1)\n",
    "\n",
    "df[[\"corp_text\", \"acadexp_text\", \"acadbg_text\"]].head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c257f684",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER) on professor profiles\n",
    "\n",
    "I use the HuggingFace `dslim/bert-base-NER` model to detect entities such as organizations (ORG)  \n",
    "and locations (LOC) in each professor profile.\n",
    "\n",
    "Design choices:\n",
    "\n",
    "- **Model**: `dslim/bert-base-NER` is a general-purpose English NER model that performs well on CV-style text.\n",
    "- **Truncation**: for each text field, I truncate to 2,000 characters to keep runtime reasonable while\n",
    "  still covering the relevant parts of long profiles.\n",
    "- **Per-section NER**: I run NER both on the full cleaned text and on each section separately, which later\n",
    "  makes it easier to attach entities to “corporate” vs. “academic” buckets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0203e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER model (dslim/bert-base-NER works well for general English entities)\n",
    "ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\")\n",
    "\n",
    "# Quick sanity check: run on one profile\n",
    "sample_text = df.loc[1, \"clean_text\"]\n",
    "ner(sample_text[:500])\n",
    "\n",
    "# Run NER on the whole cleaned profile (truncated for speed)\n",
    "df[\"entities\"] = df[\"clean_text\"].progress_apply(lambda x: ner(x[:2000]))\n",
    "\n",
    "def run_ner_safe(text: str, limit: int = 2000):\n",
    "    text = text if isinstance(text, str) else \"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    return ner(text[:limit])\n",
    "\n",
    "df[\"entities_corp\"]    = df[\"corp_text\"].progress_apply(run_ner_safe)\n",
    "df[\"entities_acadexp\"] = df[\"acadexp_text\"].progress_apply(run_ner_safe)\n",
    "df[\"entities_acadbg\"]  = df[\"acadbg_text\"].progress_apply(run_ner_safe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87879ac",
   "metadata": {},
   "source": [
    "### Normalize raw entity strings\n",
    "\n",
    "NER outputs can be noisy and inconsistent (e.g., `ie`, `IE University`, `U.K.`).  \n",
    "To reduce duplication, I:\n",
    "\n",
    "- Map common location variants (e.g., `us`, `u.s.`) to a canonical form (`United States`).\n",
    "- Map known IE-related organizations and Spanish universities to a single form (e.g., `ie university` → `IE`).\n",
    "- Drop obviously generic ORG tokens such as `Academic`, `School`, or `Faculty`.\n",
    "\n",
    "The `normalize_entities` function converts the raw NER output into clean `(type, text)` pairs  \n",
    "that are much better suited for graph construction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14060d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location and organization normalization maps\n",
    "\n",
    "LOCATION_MAP = {\n",
    "    \"us\": \"United States\",\n",
    "    \"usa\": \"United States\",\n",
    "    \"u.s.\": \"United States\",\n",
    "    \"u.k.\": \"United Kingdom\",\n",
    "    \"uk\": \"United Kingdom\",\n",
    "    \"spain\": \"Spain\",\n",
    "    \"madrid\": \"Madrid\",\n",
    "    \"mexico\": \"Mexico\",\n",
    "    \"mexico city\": \"Mexico City\",\n",
    "    \"london\": \"London\",\n",
    "    \"paris\": \"Paris\",\n",
    "    \"france\": \"France\",\n",
    "    \"germany\": \"Germany\",\n",
    "    \"italy\": \"Italy\",\n",
    "    \"portugal\": \"Portugal\",\n",
    "    \"barcelona\": \"Barcelona\",\n",
    "    \"new york\": \"New York\",\n",
    "}\n",
    "\n",
    "ORG_FIXES = {\n",
    "    # IE ecosystem\n",
    "    \"ie\": \"IE\",\n",
    "    \"ie university\": \"IE\",\n",
    "    \"ie business school\": \"IE\",\n",
    "    \"ie law school\": \"IE\",\n",
    "    \"instituto de empresa\": \"IE\",\n",
    "    \"ie school of global and public affairs\": \"IE\",\n",
    "    # Spanish universities\n",
    "    \"universidad autonoma de madrid\": \"Universidad Autónoma de Madrid\",\n",
    "    \"uam\": \"Universidad Autónoma de Madrid\",\n",
    "    \"universidad complutense de madrid\": \"Universidad Complutense de Madrid\",\n",
    "    \"universidad carlos iii de madrid\": \"Universidad Carlos III de Madrid\",\n",
    "    \"universidad politecnica de madrid\": \"Universidad Politécnica de Madrid\",\n",
    "    \"universidad de navarra\": \"Universidad de Navarra\",\n",
    "    \"universidad pontificia comillas\": \"Universidad Pontificia Comillas\",\n",
    "    \"comillas university\": \"Universidad Pontificia Comillas\",\n",
    "    \"icade\": \"Universidad Pontificia Comillas\",\n",
    "    \"iese business school\": \"IESE Business School\",\n",
    "    # companies / misc\n",
    "    \"a & am\": \"A&M Studio\",\n",
    "    \"a&m\": \"A&M Studio\",\n",
    "    \"am studio\": \"A&M Studio\",\n",
    "}\n",
    "\n",
    "GENERIC_ORG_WORDS = {\n",
    "    \"academic\",\n",
    "    \"academic exp\",\n",
    "    \"academ\",\n",
    "    \"experience\",\n",
    "    \"exp\",\n",
    "    \"university\",\n",
    "    \"universidad\",\n",
    "    \"engineering\",\n",
    "    \"design\",\n",
    "    \"academy\",\n",
    "    \"school\",\n",
    "    \"faculty\",\n",
    "    \"department\",\n",
    "    \"college\",\n",
    "    \"education\",\n",
    "    \"institute\",\n",
    "    \"business\",\n",
    "    \"finance\",\n",
    "    \"management\",\n",
    "    \"administration\",\n",
    "    \"economics\",\n",
    "    \"marketing\",\n",
    "    \"law\",\n",
    "    \"science\",\n",
    "    \"technology\",\n",
    "    \"research\",\n",
    "    \"professor\",\n",
    "    \"lecturer\",\n",
    "}\n",
    "\n",
    "def normalize_entities(entities):\n",
    "    \"\"\"Normalize tokens produced by the NER model into (type, cleaned_text) pairs.\"\"\"\n",
    "    cleaned = []\n",
    "    for ent in entities:\n",
    "        text = ent[\"word\"].lower().strip()\n",
    "        text = re.sub(r\"[^a-z0-9&.\\sáéíóúüñ]\", \"\", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        if len(text) < 3:\n",
    "            continue\n",
    "\n",
    "        if ent[\"entity_group\"] == \"LOC\":\n",
    "            text = LOCATION_MAP.get(text, text.title())\n",
    "        elif ent[\"entity_group\"] == \"ORG\":\n",
    "            text = ORG_FIXES.get(text, text.title())\n",
    "            # drop generic headings and one-word garbage\n",
    "            if text.lower() in GENERIC_ORG_WORDS or (\n",
    "                len(text.split()) == 1 and text.lower() not in [v.lower() for v in ORG_FIXES.values()]\n",
    "            ):\n",
    "                continue\n",
    "        else:\n",
    "            text = text.title()\n",
    "\n",
    "        cleaned.append((ent[\"entity_group\"], text))\n",
    "    return cleaned\n",
    "\n",
    "# Apply normalization\n",
    "df[\"normalized_entities\"] = df[\"entities\"].apply(normalize_entities)\n",
    "df[\"norm_corp\"]    = df[\"entities_corp\"].apply(normalize_entities)\n",
    "df[\"norm_acadexp\"] = df[\"entities_acadexp\"].apply(normalize_entities)\n",
    "df[\"norm_acadbg\"]  = df[\"entities_acadbg\"].apply(normalize_entities)\n",
    "\n",
    "df[\"normalized_entities\"].head(2).to_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79f7320",
   "metadata": {},
   "source": [
    "### Fuzzy clustering of organizations and locations\n",
    "\n",
    "Even after the manual maps, many entities still appear with small spelling differences  \n",
    "(e.g., accents, pluralization, or extra words). To consolidate these, I:\n",
    "\n",
    "- Normalize strings (lowercase, remove accents, drop generic words like “university”).\n",
    "- Group similar entities using token-based fuzzy matching (`rapidfuzz`).\n",
    "- Build an alias map so that near-duplicates are merged into a single canonical label.\n",
    "\n",
    "This step reduces fragmentation in the knowledge graph (fewer separate nodes for essentially the same institution).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e66d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_for_match(s: str) -> str:\n",
    "    \"\"\"Lowercase, remove accents and generic words to cluster similar entity names.\"\"\"\n",
    "    s0 = s.lower().strip()\n",
    "    s0 = \"\".join(c for c in unicodedata.normalize(\"NFKD\", s0) if not unicodedata.combining(c))\n",
    "    s0 = s0.replace(\"&\", \"and\")\n",
    "    drop = {\n",
    "        \"university\",\n",
    "        \"universidad\",\n",
    "        \"universite\",\n",
    "        \"università\",\n",
    "        \"universita\",\n",
    "        \"universidade\",\n",
    "        \"school\",\n",
    "        \"college\",\n",
    "        \"institute\",\n",
    "        \"instituto\",\n",
    "        \"dept\",\n",
    "        \"department\",\n",
    "    }\n",
    "    tokens = [t for t in re.sub(r\"[^a-z0-9\\s]\", \" \", s0).split() if t not in drop]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Collect all ORG / LOC surface forms\n",
    "all_orgs, all_locs = [], []\n",
    "for row in df[\"normalized_entities\"]:\n",
    "    for typ, val in row:\n",
    "        if typ == \"ORG\":\n",
    "            all_orgs.append(val)\n",
    "        elif typ == \"LOC\":\n",
    "            all_locs.append(val)\n",
    "\n",
    "def build_alias_map(names, sim_threshold=92):\n",
    "    \"\"\"Cluster similar names and return alias -> canonical mapping.\"\"\"\n",
    "    names = list(set(names))\n",
    "    buckets = defaultdict(list)\n",
    "    for n in names:\n",
    "        buckets[simplify_for_match(n)].append(n)\n",
    "\n",
    "    alias_map = {}\n",
    "    freq = Counter(names)\n",
    "\n",
    "    # Intra-bucket merge\n",
    "    for _, variants in buckets.items():\n",
    "        canonical = max(variants, key=lambda x: freq[x])\n",
    "        for v in variants:\n",
    "            alias_map[v] = canonical\n",
    "\n",
    "    # Inter-bucket fuzzy merge\n",
    "    canonicals = list(set(alias_map[v] for v in alias_map))\n",
    "    for c in list(canonicals):\n",
    "        matches = process.extract(c, canonicals, scorer=fuzz.token_sort_ratio, limit=5)\n",
    "        for other, score, _ in matches:\n",
    "            if other == c or score < sim_threshold:\n",
    "                continue\n",
    "            winner = c if freq[c] >= freq[other] else other\n",
    "            loser = other if winner == c else c\n",
    "            for k, v in list(alias_map.items()):\n",
    "                if v == loser:\n",
    "                    alias_map[k] = winner\n",
    "            canonicals = [winner if x == loser else x for x in canonicals]\n",
    "\n",
    "    return alias_map\n",
    "\n",
    "ORG_ALIAS = build_alias_map(all_orgs, sim_threshold=92)\n",
    "LOC_ALIAS = build_alias_map(all_locs, sim_threshold=95)\n",
    "\n",
    "def apply_aliases(entities):\n",
    "    out = []\n",
    "    for typ, val in entities:\n",
    "        if typ == \"ORG\":\n",
    "            out.append((typ, ORG_ALIAS.get(val, val)))\n",
    "        elif typ == \"LOC\":\n",
    "            out.append((typ, LOC_ALIAS.get(val, val)))\n",
    "        else:\n",
    "            out.append((typ, val))\n",
    "    return out\n",
    "\n",
    "# Apply alias maps to global and section-wise entities\n",
    "df[\"normalized_entities\"] = df[\"normalized_entities\"].apply(apply_aliases)\n",
    "df[\"norm_corp\"]    = df[\"norm_corp\"].apply(apply_aliases)\n",
    "df[\"norm_acadexp\"] = df[\"norm_acadexp\"].apply(apply_aliases)\n",
    "df[\"norm_acadbg\"]  = df[\"norm_acadbg\"].apply(apply_aliases)\n",
    "\n",
    "# Quick frequency summary\n",
    "org_counts = Counter([e[1] for row in df[\"normalized_entities\"] for e in row if e[0] == \"ORG\"])\n",
    "loc_counts = Counter([e[1] for row in df[\"normalized_entities\"] for e in row if e[0] == \"LOC\"])\n",
    "\n",
    "print(\"Top ORGs:\", org_counts.most_common(10))\n",
    "print(\"Top LOCs:\", loc_counts.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87483bf",
   "metadata": {},
   "source": [
    "### Build structured per-professor dictionaries\n",
    "\n",
    "Using the section-specific normalized entities, I construct a `professor_dict` for each row:\n",
    "\n",
    "- **Corporate Experience – Organization / Location**\n",
    "- **Academic Background – Organization**\n",
    "- Placeholder lists for **Education** and **Academic Experience** (filled later).\n",
    "\n",
    "This dictionary is the bridge between the free-text profiles and the structured knowledge graph \n",
    "we build in Notebook 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a7e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lists(section_entities):\n",
    "    orgs = [e[1] for e in section_entities if e[0] == \"ORG\"]\n",
    "    locs = [e[1] for e in section_entities if e[0] == \"LOC\"]\n",
    "    return orgs, locs\n",
    "\n",
    "def build_professor_dict_row(row):\n",
    "    corp_orgs, corp_locs       = to_lists(row[\"norm_corp\"])\n",
    "    acadexp_orgs, acadexp_locs = to_lists(row[\"norm_acadexp\"])\n",
    "    acadbg_orgs, acadbg_locs   = to_lists(row[\"norm_acadbg\"])\n",
    "\n",
    "    return {\n",
    "        \"Corporate Experience - Organization\": corp_orgs,\n",
    "        \"Corporate Experience - Location\": corp_locs,\n",
    "        \"Academic Background - Organization\": acadbg_orgs,\n",
    "        \"Academic Background - Education\": [],\n",
    "        \"Academic Experience - Courses\": [],\n",
    "        \"Academic Experience - Subjects\": [],\n",
    "        # acadexp_locs could be stored as a separate key if needed\n",
    "    }\n",
    "\n",
    "df[\"professor_dict\"] = df.apply(build_professor_dict_row, axis=1)\n",
    "df[\"professor_dict\"].head(2).to_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5bc160",
   "metadata": {},
   "source": [
    "### Extract degrees and subjects from Academic Background\n",
    "\n",
    "Some degrees and fields of study appear as free text rather than as clean NER entities.  \n",
    "To capture them, I:\n",
    "\n",
    "- Use regex patterns to match common degree formats (PhD, MBA, “Master in …”, etc.).\n",
    "- Search within the **Academic Background** slice of the profile.\n",
    "- Move these matches into `Academic Background - Education`.\n",
    "- Remove degree-like phrases that may have leaked into the organization list.\n",
    "\n",
    "This helps separate “where they studied” (ORG) from “what they studied” (degree/subject).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f41c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_section(text, start_key, stop_keys):\n",
    "    \"\"\"Return substring from `start_key` until the earliest of `stop_keys`.\"\"\"\n",
    "    t = text or \"\"\n",
    "    t_low = t.lower()\n",
    "    s = t_low.find(start_key.lower())\n",
    "    if s == -1:\n",
    "        return \"\"\n",
    "    e_candidates = [t_low.find(k.lower(), s + 1) for k in stop_keys]\n",
    "    e_candidates = [e for e in e_candidates if e != -1]\n",
    "    e = min(e_candidates) if e_candidates else len(t)\n",
    "    return t[s:e]\n",
    "\n",
    "# Degree patterns (English + some Spanish)\n",
    "DEGREE_PAT = re.compile(\n",
    "    r\"\"\"\n",
    "    \\b(\n",
    "        ph\\.?d\\.?|doctor(?:ate)?\\s+of\\s+[A-Za-zÁÉÍÓÚÜÑ&\\-\\s]+|\n",
    "        m\\.?b\\.?a\\.?|m\\.?sc\\.?|m\\.?s\\.?|m\\.?a\\.?|ll\\.?m\\.?|\n",
    "        b\\.?sc\\.?|b\\.?s\\.?|b\\.?a\\.?|\n",
    "        master(?:'s)?\\s+in\\s+[A-Za-zÁÉÍÓÚÜÑ&\\-\\s]+|\n",
    "        bachelor(?:'s)?\\s+in\\s+[A-Za-zÁÉÍÓÚÜÑ&\\-\\s]+|\n",
    "        licenciatura\\s+en\\s+[A-Za-zÁÉÍÓÚÜÑ&\\-\\s]+|\n",
    "        grado\\s+en\\s+[A-Za-zÁÉÍÓÚÜÑ&\\-\\s]+\n",
    "    )\\b\n",
    "    \"\"\",\n",
    "    re.IGNORECASE | re.VERBOSE,\n",
    ")\n",
    "\n",
    "# Subject extractor: \"... in X\" or \"... of X\"\n",
    "SUBJECT_PAT = re.compile(r\"\\b(?:in|of)\\s+([A-Z][A-Za-zÁÉÍÓÚÜÑ&\\-\\s]{3,})\")\n",
    "\n",
    "def split_background_fields(row):\n",
    "    bg_text = get_section(\n",
    "        row.get(\"clean_text\", \"\"),\n",
    "        start_key=\"Academic Background\",\n",
    "        stop_keys=[\"Academic Experience\", \"Corporate Experience\"],\n",
    "    )\n",
    "\n",
    "    degrees = [m.group(0).strip().rstrip(\",.;\") for m in DEGREE_PAT.finditer(bg_text)]\n",
    "    subjects = [m.group(1).strip().rstrip(\",.;\") for m in SUBJECT_PAT.finditer(bg_text)]\n",
    "\n",
    "    d = row[\"professor_dict\"].copy()\n",
    "    ab_orgs = d.get(\"Academic Background - Organization\", [])\n",
    "    cleaned_ab_orgs = []\n",
    "    for org in ab_orgs:\n",
    "        if DEGREE_PAT.search(org) or SUBJECT_PAT.search(org):\n",
    "            continue\n",
    "        cleaned_ab_orgs.append(org)\n",
    "\n",
    "    d[\"Academic Background - Organization\"] = cleaned_ab_orgs\n",
    "    d[\"Academic Background - Education\"] = sorted(\n",
    "        set(d.get(\"Academic Background - Education\", []) + degrees)\n",
    "    )\n",
    "    d[\"Academic Experience - Subjects\"] = sorted(\n",
    "        set(d.get(\"Academic Experience - Subjects\", []) + subjects)\n",
    "    )\n",
    "\n",
    "    return d\n",
    "\n",
    "df[\"professor_dict\"] = df.apply(split_background_fields, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf43a66",
   "metadata": {},
   "source": [
    "### Clean and finalize professor dictionaries\n",
    "\n",
    "The raw buckets can still contain noise (section headers, conjunctions, etc.).  \n",
    "The `finalize_professor_dict` step:\n",
    "\n",
    "- Removes short or obviously junk tokens.\n",
    "- Normalizes common variants such as “I E University” → “IE”.\n",
    "- De-duplicates each list.\n",
    "- Applies a fallback: if all buckets are empty for a professor, it falls back to the global\n",
    "  NER entities so that the profile still contributes to the graph.\n",
    "\n",
    "This produces a compact but robust `professor_dict` for each profile.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c3956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUNK_TOKENS = {\n",
    "    \"academic ba\",\n",
    "    \"academic back\",\n",
    "    \"of\",\n",
    "    \"and\",\n",
    "    \"school\",\n",
    "    \"university\",\n",
    "    \"ll. m\",\n",
    "    \"& am\",\n",
    "    \"& amp\",\n",
    "    \"research\",\n",
    "    \"experience\",\n",
    "    \"business\",\n",
    "    \"administration\",\n",
    "}\n",
    "JUNK_RE = re.compile(r\"^(?:&|of|and|the|section|experience|academic|school|university)\\b\", re.I)\n",
    "\n",
    "def _clean_list(vals):\n",
    "    out, seen = [], set()\n",
    "    for v in vals:\n",
    "        v_norm = re.sub(r\"\\s+\", \" \", v).strip()\n",
    "        v_low = v_norm.lower()\n",
    "        if len(v_norm) < 3:\n",
    "            continue\n",
    "        if v_low in JUNK_TOKENS or JUNK_RE.match(v_norm):\n",
    "            continue\n",
    "        if v_norm.lower().replace(\" \", \"\") in {\"ieuniversity\", \"ie\"}:\n",
    "            v_norm = \"IE\"\n",
    "        if v_norm not in seen:\n",
    "            out.append(v_norm)\n",
    "            seen.add(v_norm)\n",
    "    return out\n",
    "\n",
    "def finalize_professor_dict(row):\n",
    "    d = row[\"professor_dict\"].copy()\n",
    "    d[\"Corporate Experience - Organization\"] = _clean_list(\n",
    "        d.get(\"Corporate Experience - Organization\", [])\n",
    "    )\n",
    "    d[\"Corporate Experience - Location\"] = _clean_list(\n",
    "        d.get(\"Corporate Experience - Location\", [])\n",
    "    )\n",
    "    d[\"Academic Background - Organization\"] = _clean_list(\n",
    "        d.get(\"Academic Background - Organization\", [])\n",
    "    )\n",
    "    d[\"Academic Background - Education\"] = _clean_list(\n",
    "        d.get(\"Academic Background - Education\", [])\n",
    "    )\n",
    "    d[\"Academic Experience - Subjects\"] = _clean_list(\n",
    "        d.get(\"Academic Experience - Subjects\", [])\n",
    "    )\n",
    "    d[\"Academic Experience - Courses\"] = _clean_list(\n",
    "        d.get(\"Academic Experience - Courses\", [])\n",
    "    )\n",
    "\n",
    "    # Fallback: if everything is empty, use global NER buckets\n",
    "    if not any(d[k] for k in d.keys()):\n",
    "        ents = row[\"normalized_entities\"]\n",
    "        d[\"Corporate Experience - Organization\"] = _clean_list([e[1] for e in ents if e[0] == \"ORG\"])\n",
    "        d[\"Corporate Experience - Location\"] = _clean_list([e[1] for e in ents if e[0] == \"LOC\"])\n",
    "        d[\"Academic Background - Organization\"] = _clean_list([e[1] for e in ents if e[0] == \"ORG\"])\n",
    "\n",
    "    return d\n",
    "\n",
    "df[\"professor_dict\"] = df.apply(finalize_professor_dict, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca13be",
   "metadata": {},
   "source": [
    "### Re-bucket entities based on section-specific text\n",
    "\n",
    "To improve attribution, I run a final pass where I:\n",
    "\n",
    "- Re-run NER on just the **Corporate Experience** and **Academic Background** slices.\n",
    "- Use these section-specific entities to overwrite the earlier organization/location buckets.\n",
    "\n",
    "This strengthens the link between each entity and the part of the CV where it appears.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed8b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def section_entities(text, start_key, stop_keys, maxlen=2000):\n",
    "    s = get_section(text, start_key, stop_keys)\n",
    "    if not s:\n",
    "        return []\n",
    "    ents = ner(s[:maxlen])\n",
    "    return normalize_entities(ents)\n",
    "\n",
    "def rebuild_from_sections(row):\n",
    "    d = row[\"professor_dict\"].copy()\n",
    "    txt = row.get(\"clean_text\", \"\")\n",
    "\n",
    "    corp_ents = section_entities(\n",
    "        txt, \"Corporate Experience\", [\"Academic Background\", \"Academic Experience\"]\n",
    "    )\n",
    "    acad_bg_ents = section_entities(\n",
    "        txt, \"Academic Background\", [\"Corporate Experience\", \"Academic Experience\"]\n",
    "    )\n",
    "\n",
    "    d[\"Corporate Experience - Organization\"] = sorted({n for t, n in corp_ents if t == \"ORG\"})\n",
    "    d[\"Corporate Experience - Location\"] = sorted({n for t, n in corp_ents if t == \"LOC\"})\n",
    "    d[\"Academic Background - Organization\"] = sorted({n for t, n in acad_bg_ents if t == \"ORG\"})\n",
    "    return d\n",
    "\n",
    "df[\"professor_dict\"] = df.apply(rebuild_from_sections, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb75cc7",
   "metadata": {},
   "source": [
    "### Final noise filtering\n",
    "\n",
    "As a last step, I filter out leftover headings and geographic placeholders that slipped into\n",
    "organization lists. This keeps the organization nodes focused on institutions and companies,\n",
    "not on countries or section labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f77af60",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUNK_ORG = re.compile(\n",
    "    r\"^(academic( back| exp).*$|of excellence$|section \\d+(st|nd|rd|th)$|\"\n",
    "    r\"(journal|conference|school of|law school|business school)$|\"\n",
    "    r\"&\\s*am?p?$|watkins ll?$|ll\\.?$)\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "PLACE_WORDS = {\"Spain\", \"Madrid\", \"Paris\", \"London\", \"Italy\", \"United States\", \"Europe\"}\n",
    "\n",
    "def clean_prof_dict(d):\n",
    "    def filt_org(lst):\n",
    "        out = []\n",
    "        for x in lst:\n",
    "            if len(x) < 3:\n",
    "                continue\n",
    "            if JUNK_ORG.search(x):\n",
    "                continue\n",
    "            out.append(x)\n",
    "        return sorted(set(out))\n",
    "\n",
    "    d[\"Corporate Experience - Organization\"] = filt_org(\n",
    "        d.get(\"Corporate Experience - Organization\", [])\n",
    "    )\n",
    "    d[\"Academic Background - Organization\"] = filt_org(\n",
    "        d.get(\"Academic Background - Organization\", [])\n",
    "    )\n",
    "    d[\"Academic Experience - Subjects\"] = [\n",
    "        s for s in d.get(\"Academic Experience - Subjects\", []) if s not in PLACE_WORDS\n",
    "    ]\n",
    "    return d\n",
    "\n",
    "df[\"professor_dict\"] = df[\"professor_dict\"].apply(clean_prof_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a8c545",
   "metadata": {},
   "source": [
    "### Sanity check and export cleaned data\n",
    "\n",
    "Before exporting, I sample a subset of professors and print their dictionaries to verify that:\n",
    "\n",
    "- Corporate and academic organizations look reasonable.\n",
    "- Degrees and subjects are being captured correctly.\n",
    "- Obvious noise has been removed.\n",
    "\n",
    "Finally, I save:\n",
    "\n",
    "- A full parquet file (`teachers_db_cleaned.parquet`) with all intermediate columns.\n",
    "- A compact JSON file (`cleaned_professors.json`) containing only the cleaned text and `professor_dict`.\n",
    "\n",
    "The JSON file is the input to Notebook 2, where I build the professor knowledge graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726d848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dct in enumerate(df[\"professor_dict\"].sample(30, random_state=43).to_list(), 1):\n",
    "    print(f\"\\n--- Professor {i} ---\")\n",
    "    for k, v in dct.items():\n",
    "        print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e1c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "# Parquet with full intermediate data (useful for debugging or further analysis)\n",
    "parquet_path = \"../data/processed/teachers_db_cleaned.parquet\"\n",
    "df.to_parquet(parquet_path, index=False)\n",
    "print(f\"Saved cleaned parquet to {parquet_path}\")\n",
    "\n",
    "# Compact JSON for Notebook 2: only the text + structured dictionary\n",
    "json_path = \"../data/processed/cleaned_professors.json\"\n",
    "df[[\"clean_text\", \"professor_dict\"]].to_json(\n",
    "    json_path,\n",
    "    orient=\"records\",\n",
    "    indent=2,\n",
    "    force_ascii=False,\n",
    ")\n",
    "print(f\"Saved cleaned JSON for graph building to {json_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
